{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめのまとめのまとめ\n",
    "交差検定は大事やで！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1章 Pythonではじめる機械学習\n",
    "\n",
    "- データを理解し，データを扱いやすい形に整形することは大事\n",
    "- アルゴリズムの出した結果に対して，正しい評価を行うことが大事\n",
    "    - 訓練データとテストデータを区別したり\n",
    "- モデルとは，複雑な現実世界で起こる現象を理論的に単純化して近似したもの\n",
    "    - モデルと実際のデータの誤差を小さくし，かつ未知のデータにも正しい答えを出力できるようにするのが目的\n",
    "- 多項式フィッティングを使ってWebトラフィックの予測を行なった．\n",
    "    - 次数を大きくしすぎると過学習を起こす\n",
    "    - データの特徴から，フィッティング曲線を時間毎に分けてみることも一つの手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2章 実例を対象とした分類法入門\n",
    "クラス分類の基本的な話\n",
    "- データセット\n",
    "    - Irisデータセット： 小規模で広く使われている\n",
    "    - Seedsデータセット： 小麦の産地を7つの特徴量から推定する\n",
    "- 分類モデル\n",
    "    - 閾値を使ったモデル: 人間の知識やデータの考察が必要\n",
    "    - 最近傍法: 訓練特徴量をベクトルとし，もっとも近いN次元上の点のラベルを正解とする手法\n",
    "    - K近傍法: もっとも近いK個の点のラベルの多数決の結果を正解とする手法\n",
    "- 評価手法\n",
    "    - 訓練データをモデルの評価に使ってはいけない\n",
    "    - 交差検定\n",
    "        - leave-one-out: データの一つを取り除いてそれ以外のデータで訓練した結果，取り除いたデータに対し正しい結果を返すかを確かめる\n",
    "        - K分割交差検定: データをK個に分割し，それぞれのブロックを取り除いて訓練した結果，取り除いたブロックに対しどれだけ正しい結果を返すかを確かめる\n",
    "- 特徴量について\n",
    "    - 洗練されたアルゴリズムより良いデータ，良い特徴量\n",
    "    - 特徴選択： 良く分類ができる特徴量を選択する手法\n",
    "    - 特徴エンジニアリング: 良く分類ができるような特徴量を，現存する特徴量から計算で作り出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3章 クラスタリング 関連のある文書を見つける\n",
    "\n",
    "関連する文書を素早く見つけるため，文書のクラスタリング(教師なし学習)を行った．\n",
    "\n",
    "- 文書の前処理\n",
    "    - bag-of-words化: 単語の出現回数を数え，その文書の特徴とする，Scikit-Learnでできる\n",
    "        - 同じ文章が繰り返されているだけで単語の重要度が上がってしまうので，出現回数の正規化を行う\n",
    "        - min_df, max_df: 滅多に出現しない単語，非常に多く一般的に使われる単語を取り除く\n",
    "        - stop words: その言語体系で文書の分類に貢献しないような一般的な単語を取り除く\n",
    "        - ステミング: 語幹を数える (ex. imaging -> image)\n",
    "        - TF-IDF: その単語がどれだけすべての文書の中で特徴的かを表す指標，Scikit-Learnで算出できる\n",
    "    - N-gram: not eatのように否定を表したりすることを認識させたいならば，前後するペア(バイグラム)や3つの単語(トリグラム)を利用する\n",
    "- クラスタリング手法\n",
    "    - 新しい文書を分類するとき，bag-of-wordsのノルムを見れば良いだけになり，素早い分類が可能になる\n",
    "    - フラットクラスタリング: 全てのデータが高々１つのクラスタに所属\n",
    "        - KMeans: 広く使われるクラスタリング手法\n",
    "            1. クラスタの数num_clustersを指定して初期化\n",
    "            1. num_clustersの数だけランダムにデータを選び出し，それらの特徴ベクトルを各クラスタの中心点とする\n",
    "            1. その他のデータは，もっとも近い中心点を持つクラスタに所属させる\n",
    "            1. 全てのデータのクラスタが決まったら，各クラスタの中心を新たに計算し，そこに中心点を移動させる\n",
    "            1. 中心点の移動量が閾値を下回る(収束)，または指定の繰り返し回数に達するまで2~4を繰り返す\n",
    "    - 階層的クラスタリング: クラスタのクラスタ，のように階層構造を持つ\n",
    "    - コサイン類似度，ピアソンの相関係数，Jaccard係数などの他の類似度の指標を使ったりしてマイナーチェンジしてみてもいい\n",
    "    - 良いクラスタリングの指標を明確にするのが大事．代表的な指標はsklearn.metrixにある．\n",
    "\n",
    "文書のクラスタリングをするときの流れ\n",
    "1. 文書データを集める\n",
    "1. 文書データをScikit-LearnでVectorizeする\n",
    "1. Vectorizeされた文書をScikit-LearnのKMeansなどでクラスタリング\n",
    "1. 新しい文書データを分類する\n",
    "    1. 新しい文書データをVectorizeする\n",
    "    1. クラスタリング結果を持つKMeansに文書をPredictさせると，新しい文書データの所属するクラスタが素早くわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4章 トピックモデル\n",
    "トピックモデル: データの一つ一つを「複数の」グループに割り当てる問題を扱う分野．で，文書データを分類\n",
    "- LDA(潜在的ディリクレ配分法, Latent Dirichlet Allocation): 最も単純なトピックモデル\n",
    "    - テキスト分類を考える．\n",
    "        - すべての単語に，関連するトピックを割り当てる．\n",
    "        - トピックの正体は単語についての多項分布で，各単語とそのトピックとの関連の深さを確率で表現している．\n",
    "        - ドキュメントに入っている単語のトピック強度の総和がそのドキュメントのトピック強度になる．  \n",
    "        - LDAは文章生成モデルで，あるトピックを選ぶとそのトピックのドキュメントが最もよく生成されるようにフィッティングする．\n",
    "    - 例えば，オバマと書いてあるドキュメントとトランプと書いてあるドキュメントを，トピックが似ている，と分析できる．\n",
    "    - gensimパッケージでLDAを使える．\n",
    "        - コーパスとid2wordを与えてやると学習完了．モデルにドキュメントを与えると関連するトピックが返ってくる．\n",
    "        - alphaで一つのドキュメントに割り当たるトピックの数が変わる．\n",
    "        - あるトピックが何のトピックであるのかは，そのトピックのトピック強度が強い単語を見て考えてやる．\n",
    "        - 前処理(ステミングや無関係文字の削除)を行うとなおよい\n",
    "        - alphaやトピック数といったパラメータを変えても，最終的な結果にはあまり影響がない．\n",
    "    - 学習後のモデルそのままでもwordcloudによる可視化などで役に立つ．\n",
    "    - 次元削減の効果があり，他の手法の中間的な役割を担うこともできる．\n",
    "    - 各ドキュメントのすべてのトピック強度をベクトルとすると，最もトピックが近い文章を求めることができる．\n",
    "- 階層ディリクレ過程(hierarchical Dirichlet process: HDP): トピック数をデータセットに応じて自動決定するトピックモデル\n",
    "    - 直観的には，文章が多ければ多いほど多くのトピックを得ることを利用する．\n",
    "    - 例えば，ニュースの記事データが少なければトピックは「スポーツ」になるかもしれないが，多ければ「サッカー」などと細かくできる．\n",
    "    - この手法はgensimに用意されており，先ほどのLDAをHDPに変えるだけで実現できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 クラス分類 悪い回答を判別する\n",
    "\n",
    "ノイズの多いテキストデータを良い回答，悪い回答に分類するにはどうすれば良いか？という問題に取り組んだ．  \n",
    "使用したデータはstackoverflowの問答データ．教師データになりそうなのはScore．  \n",
    "以降の分類器はscikit-learnに便利な関数が多数用意されている．  \n",
    "\n",
    "- K近傍法による分類\n",
    "    - 特徴量を増やしたり，Kの値を大きくしてモデルを単純にしても，時間がかかるだけで精度が上がらないことがある．  \n",
    "    - モデルを単純にしても特徴量を改良してもバリアンスが高いので，K近傍法はこのタスクに向かないとわかった．\n",
    "        - バイアス-バリアンストレードオフ\n",
    "            - (エラーの)バイアスが高い->未学習, 訓練誤差もテスト誤差も大きい, データのノイズが多い， 特徴量の設計がマズい\n",
    "            - バリアンスが高い->過学習, 訓練誤差とテスト誤差の差が大きい, モデルが複雑すぎる\n",
    "            - これらはトレードオフであり，最適解を探したい\n",
    "- ロジスティック回帰による分類\n",
    "    - ロジスティック回帰: どんな入力にも白か黒かの確率を返す関数の回帰による最適化\n",
    "        - 要は「オッズ比の対数関数の逆関数に入力する線形式の最適化」\n",
    "        - オッズ比：$P/(1-P)$\n",
    "        - 入力無限領域，出力0~1の確率の形にできる\n",
    "        - 1次式を入力するシグモイド関数とも言える\n",
    "        - バイアス+ 係数 ＊ 特徴量の値 + 係数 ＊ 特徴量の値 + ... をロジスティック回帰の式に入力することでクラスに属する確率を得る\n",
    "        - 正規化パラメータCがある\n",
    "    - 最適なCを選んでも，90NNと同じくらいの精度しか出ない．\n",
    "        - バイアスが大きい\n",
    "    - 悪い回答の確率と良い回答の確率をそれぞれ求めるより，片方を求めてもう片方はあまり，としたときの精度の方が役に立つのでは？\n",
    "        - 適合率-再現率曲線(Precision-Recall curve)のAUC(Area Under Curve)がもっとも大きい閾値を求める\n",
    "            - 適合率： $TP / (TP + FP)$ 冤罪を起こさない確率\n",
    "            - 再現率: $ TP / (TP + FN) $ 犯人を逃さない確率\n",
    "            - 適合率と再現率はトレードオフ． 再現率をあげてもどれだけ適合率が下がらないかをグラフでみるのが適合率-再現率曲線\n",
    "    - 結果として，良い回答かどうかを分類する方が，悪い回答かどうかを分類するより高いAUCを出せることがわかった．  \n",
    "    - ロジスティック係数が0に近い特徴量は分類器の精度にあまり影響を与えないので省ける\n",
    "    - このモデルをで保存しておくことで，Webサービスなどでは素早く回答の良し悪しを判別させられる．\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6章 クラス分類Ⅱ 感情分析\n",
    "ツイートのポジティブ/ネガティブ分類器を作る．  \n",
    "- ナイーブベイズ\n",
    "    - 分類したいクラスを$C$,特徴量を$F_1, F_2$とするとベイズの定理から$$ P(C | F_1, F_2) = \\frac{P(C) \\cdot P(F_1, F_2 | C)}{P(F_1, F_2)} $$\n",
    "    - $P(C | F_1, F_2)$: 事後確率 $F_1$と$F_2$がわかっているとき，そのデータがクラス$C$に属する確率\n",
    "    - $P(C)$: 事前確率, データについて何も情報がない場合にそのデータがクラス$C$に属する確率\n",
    "    - $P(F_1, F_2 | C)$: 尤度, あるデータがクラス$C$に属することがわかっている場合，特徴量がその$F_1，F_2$である確率\n",
    "    - $P(F_1, F_2)$: 証拠, 特徴量がその$F_1$と$F_2$をとる確率, 該当する特徴量が全体に占める割合を計算して求める\n",
    "    - これよりクラスの判定結果は$$ C_{best} = \\arg \\max_{c \\in C} P(C=c) \\cdot P(F_1 | C=c) \\cdot P(F_2 | C=c) $$\n",
    "    - 新出データに対して確率を求めると証拠が0になってしまうので，全ての単語が1度は出ていることを仮定するスムージングを行う．\n",
    "        - 加算スムージング(ラプラススムージング): 全ての頻度に1を足す\n",
    "        - Lidstoneスムージング: 0以上のパラメータ$\\alpha$を足す\n",
    "    - 実用上は，限りなく小さい確率の積によるアンダーフローを防ぐため，積を和に変換できる対数をとる．この場合，\n",
    "    $$ C_{best} = \\arg \\max_{c \\in C} \\log P(C=c) + \\sum_k \\log P(F_k | C=c) $$\n",
    "    - sklearnには3つのナイーブベイズ分類器が用意されている． 今回はMultinomialNBを利用．\n",
    "        - GaussianNB: 特徴量が正規分布(ガウス分布)にしたがって分布すると仮定, 身長と体重から性別を分類する問題など\n",
    "        - MultinomialNB: ある事象が発生した回数を特徴量としていると仮定, TF-IDFと相性がよい\n",
    "        - BernoulliNB: 単語が出現した / 出現していない の2値で特徴量が表される場合に適している．  \n",
    "<br>\n",
    "- ポジティブとネガティブを判定する分類器\n",
    "    - TfidfVectorizer->MultinomialNB というモデルを Pipelineで繋いだモデルで分類．\n",
    "    - 以降，データが少ないので，交差検定を行う．ShuffleSplitでテストデータと訓練データを分ける．\n",
    "    - 正解率80%, AUC(Area Under precision-recall curve)88%(以下80/88のように表記)の良い感じの結果に．  \n",
    "<br>\n",
    "- 全てのクラスを判定する分類器\n",
    "    - 感情を含む / 含まない: 79/67: 感情を含むツイートは2割なので役立たず\n",
    "    - ポジティブ/ その他: 90/27: PRカーブの成績が悪い\n",
    "    - ネガティブ/ その他: 88/47: PRカーブの成績が悪い\n",
    "    - データに偏りがあるのでPRカーブを信じるべき．するとこの結果はあまり良くない．  \n",
    "<br>\n",
    "- 感情を含む / 含まないの判定についてパラメータ最適化を行なった分類器\n",
    "    - GridSearchCVによって交差検定(Cross Validation)の結果が最も良いパラメータを全探索\n",
    "        - Param_grid = {\"vect__ngram_range\": [(1,1), (1,2), (1,3)]} のような辞書を作る．\n",
    "    - 交差検定の指標にはF値(metrics.f1_scoreで実装)を用いる．$$ F = \\frac{2 \\cdot precision \\cdot recall}{precision + recall} $$\n",
    "        - precisionとrecallの調和平均(逆数の平均の逆数,率の平均を取るのに適する,値の小さい方に引っ張られる平均)\n",
    "    - 感情を含む / 含まない: 83/70 ちょっと良くなった  \n",
    "<br>\n",
    "- ツイートの整形を行う分類器(パラメータ最適)\n",
    "    - TfidfVectorizerにpreprocessorを渡す．\n",
    "        - 顔文字を単語に変換\n",
    "        - 短縮系の言葉を単語に分割\n",
    "    - pos vs neg: 80/88, emo vs rest: 83/70, pos vs rest: 91/51, neg vs rest: 90/65  \n",
    "<br>\n",
    "- 単語のポジネガスコアを利用する分類器(パラメータ最適，整形済み)\n",
    "    - nltk.pos_tag(nltk.word_tokenize(\"This is a sentense.\")) による単語品詞タグづけ(Penn Treebankプロジェクト)\n",
    "    - SentiWordNetのデータによる「品詞/単語」のポジネガスコアを獲得し，特徴量とする\n",
    "    - BaseEstimatorを継承して，ポジネガスコアを特徴量として算出するクラスを作り，TfidfVectorizerの特徴量とFeatureUnionによって合わせる．\n",
    "    - pos vs neg: 80/88, emo vs rest:83/69, pos vs rest: 91/52, neg vs rest: 89/62, 努力の割にはそんなに変わらず\n",
    "    - まず感情があるかないかを分類してからポジネガ分類器にかけると良いかもしれない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7章 レコメンド\n",
    "\n",
    "- 最小二乗法の復習\n",
    "    - sklearnのボストン物件価格データを利用\n",
    "    - 特徴量一つだけよりバイアスを加えた方が，さらにたくさん特徴量があった方が一般的に良い精度が出る．\n",
    "    - 回帰分析でも交差検定をして正しい汎化能力を確かめよう．\n",
    "    - 特徴量の数P > サンプル数N のとき，訓練誤差をほぼゼロにすることができるが，汎化能力が著しく落ちる．\n",
    "        - 10-K reportsのデータはP>Nとなっており，これでP>N問題を確かめた．\n",
    "    - 過学習対策\n",
    "        - Lasso: L1罰則項(回帰係数の絶対値の総和)をペナルティとする．\n",
    "            - スパースなモデルに．\n",
    "        - Ridge: L2罰則項(回帰係数の2乗の総和)をペナルティとする．\n",
    "        - Elastic Net: L1, L2の比率を設定し，それらの和を罰則項とする． \n",
    "            - P>N問題の解消，相関の高い特徴量からの複数考慮といった効果がある．\n",
    "- パラメータ探索には2段階の交差検定が必要\n",
    "    - 1段目： テストデータと訓練データに分ける\n",
    "    - 2段目： 1段目の訓練データをさらに分割し，それぞれ違ったパラメータを与えて訓練した結果，誤差が最少となったパラメータを採用\n",
    "- 映画の推薦問題を罰則付き回帰分析で解いてみる\n",
    "    - Netflix Challengeのアカデミック用データを利用\n",
    "    - ターゲットユーザ以外のユーザが行なった評価を特徴量としてLassoで解いてみた\n",
    "    - 全体の平均をスコアの予測値とするよりはLassoを使った方が8％ほどの精度改善が見られた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8章 レコメンドの改良\n",
    "- 映画の推薦システムの改良\n",
    "    - スコアではなく，どの映画にスコアをつけたかに注目\n",
    "        - スコアをつけた映画が似ているユーザのスコアを予測値とする手法\n",
    "            - 各ユーザの相関係数を利用する\n",
    "        - 逆に，スコアをつけたユーザが似ている映画に着目してみる手法\n",
    "    - アンサンブル学習: 上記の2つの手法のような，複数の手法を組み合わせて予測を行う手法\n",
    "        - 各手法の予測結果は新たな特徴量と考えられるので，回帰によって重み付けを行える．\n",
    "            - 重み付けの結果，重みの小さな特徴量は役に立たず，全体のパフォーマンスを下げる可能性もある．\n",
    "            - 複数の手法を簡単に比較検討する手段としても使える．\n",
    "- バスケット分析: 「この商品を買った人はこんな商品も買っています」\n",
    "    - どの商品が一緒に買われているかに着目する．\n",
    "        - あるユーザが複数の商品を買っている時，そのユーザが他にどんなものを買う傾向があるのかを予測できる．  \n",
    "        - 生活必需品など，多くの人が買わざるを得ない商品はユーザの趣味嗜好に関係がないので注意する．\n",
    "    - ベルギーのスーパーマーケットでの匿名トランザクションデータ, retail.datを使って実験\n",
    "    - 1. 頻出アイテム集合の列挙\n",
    "        - アプリオリアルゴリズム: 頻出アイテム集合: 頻繁に購入される商品の組み合わせ をトランザクション集合から列挙する．\n",
    "            - 購入頻度(支持度)が閾値(最小支持度)以上の商品1つによる集合を頻出アイテム集合とするところから始める\n",
    "            - 頻出アイテム集合に一つ頻出アイテムを追加し，その支持度を求め, それがまだ最小支持度以上ならば頻出アイテム集合として記録していく\n",
    "            - 新たな頻出アイテム集合がどのトランザクションに含まれているかをキャッシュすることによって高速化することができる．\n",
    "    - 2. アソシエーションルールマイニング\n",
    "        - 頻出アイテム集合の列挙を元に，アソシエーションルールの強さを導く\n",
    "            - アソシエーションルール: 「この商品の組みあわせXを買っている人はこの商品Yを買っている傾向がある」\n",
    "        - アソシエーションルールの強さの指標として，次のLIFT値を用いる. 誰もが買っている商品に対するペナルティを課している．\n",
    "            $$ lift(X \\Rightarrow Y) = \\frac{P(Y | X)}{P(Y)} $$\n",
    "        - 1で作成したキャッシュを利用することで高速化できる．\n",
    "    - 他にも，買い物の順序を考慮した手法などが，pyminingというパッケージで提供されている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9章 クラス分類Ⅲ 音楽ジャンル分類\n",
    "音楽の複数ジャンル分類問題\n",
    "- GTZANというデータセットを利用\n",
    "    - 10個のジャンルのうち6個を今回は使用\n",
    "- FFTによる分類: あまりうまくいかない\n",
    "    - FFT, 高速フーリエ変換: 波形データを周波数の違うサイン波に分解し，周波数ごとの強度を可視化する\n",
    "        - バタフライ演算による対数オーダーの高速化\n",
    "    - ロジスティック回帰によるジャンル分類\n",
    "        - 多値分類問題にも適用できるが，2値分類問題として学習させた方がよさげ\n",
    "    - 混同行列: どのジャンルが正解のとき，どのジャンルに何曲分類されたかを可視化できる．対各成分が濃いと良い．\n",
    "        - matplotlibのmatshow関数で可視化できる\n",
    "    - ROC(receiver operator characteristic)曲線による評価\n",
    "        - false positive rateとtrue positive rateによる曲線，こちらもP/R曲線と同様，AUCが高いほど良い\n",
    "        - P/R曲線は陽性サンプルが貴重なとき有効なのに対し，ROCは分類器の振る舞いについて全体像を見たいときに利用する．\n",
    "- メル周波数ケプストラム係数(MFCC)による分類: 割とうまくいく\n",
    "    - MFCC\n",
    "        - 音声の波形データを低次元に落とし込む「手法」\n",
    "        - 波形→離散フーリエ変換→対数を取る→メルフィルタバンクをかける→離散コサイン変換→低次の係数を取得\n",
    "        - libsoraパッケージで波形からMFCCを得る関数を利用可能\n",
    "    - ROC曲線のAUC,混同行列の対角線の濃さ共に全体的に良好だが，カントリーやジャズの分類は難しい模様"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10章 コンピュータビジョン パターン認識\n",
    "- 基本的な画像処理 by mahotas\n",
    "    - 閾値処理(二値化)\n",
    "        - 大津の二値化: 高めの閾値，大まかな分離をさせたい時に有効\n",
    "        - Ribler-Calvardの二値化: 低めの閾値，二値化しても細部を残したい時に有効\n",
    "        - open, closeでノイズ除去\n",
    "    - ガウシアンぼかし\n",
    "        - 標準偏差を大きくするほど強くボケるフィルタ\n",
    "        - ぼかしてから二値化すると大きめのノイズも除去できる\n",
    "    - フィルタ処理\n",
    "        - ソルト&ペッパーノイズ: スキャナーのシミュレーションやロバストなデータ作りに\n",
    "        - 周囲をぼかす： ガウス分布で中心から外側に行くほどガウシアンぼかしを行なった画像の要素を強くする\n",
    "- シンプルなデータセットのパターン認識\n",
    "    - パターン認識: 画像のクラス分類\n",
    "    - Haralickテクスチャ特徴量: 風景のような滑らかな画像とテキストのような明暗がはっきりした画像の分類\n",
    "        - ロジスティック回帰で72%\n",
    "    - LInear Binary Patterns特徴量: 線形演算を行なっても普遍なので照明の変化にロバスト\n",
    "    - ソーベルフィルタを使った特徴量:\n",
    "        - 水平方向のフィルタと垂直方向のフィルタでエッジ検出を行う．\n",
    "        - 検出されたエッジの内積による数値化によって大域特徴量にする．\n",
    "        - Haralickと合わせて84%\n",
    "- 難しいデータセットのパターン認識\n",
    "    - テクスチャ特徴量だと52%\n",
    "    - 局所特徴量による記述\n",
    "        - 画像の局所的な領域を対象に計算される特徴量\n",
    "        - ランダム， グリッド， 特徴領域検出といった手法で特徴量を得る領域を指定\n",
    "        - SURF(Speeded Up Robust Features), SIFT(Scale-Invariant FeatureTransform)など\n",
    "            - descriptor(記述子, 画像中の対象領域を特徴ベクトルに変換した結果)を得る\n",
    "        - descriptorにbag-of-wordsを適用する\n",
    "            - descriptorをクラスタリングにかけることで，似たような音声を同じ単語と認識するように，画像のクラス分けができる\n",
    "            - クラスター，画像の単語の数のようなものは，極端に大きかったり，小さ買ったりしないのが望ましい．\n",
    "        - SURF: 61%, SURF+sobel+haralick: 65%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11章 次元削減\n",
    "不必要な特徴量を削除することで，テスト精度向上，高速化，可視化などのメリットが得られる．  \n",
    "\n",
    "- 特徴選択\n",
    "    - フィルター法: 関連性の強い特徴量を削除\n",
    "        - ピアソンの相関係数を関連性の指標とする\n",
    "            - 線形な関連性しか見れない\n",
    "            - P値： その相関係数が偶然である確率，信頼性の低さ\n",
    "        - 相互情報量を関連性の指標とする\n",
    "            - 相互情報量： 片方がわかったとき，もう片方のエントロピー(不確実性)をどれだけ減らすか→共依存度\n",
    "            - 非線形な関連性も見れる\n",
    "        - 特徴量の全てのペアの組み合わせを見ていると大変時間がかかる(2乗オーダ)\n",
    "    - ラッパー法: 分類器をラッピングし，訓練しながら影響の小さな特徴量を削除していく\n",
    "        - モデルの視点からは不必要な特徴量がまだあったり，複数の特徴量を組み合わせることで有用になることがあることに着目\n",
    "        - RFE(Recursive Feature Elimination, 再帰的特徴削減)が主要な手法\n",
    "    - 決定木やL1ノルムのように，特徴選択は学習プロセスの中に含まれていることがある．\n",
    "- 特徴抽出\n",
    "    - 特徴選択よりさらに削減したい場合や，可視化したい場合に行う\n",
    "    - 主成分分析(Principal Component Analysis: PCA) まずはコレ！\n",
    "        - 高速で逆変換可能(逆変換時の誤差を最小にする）\n",
    "        - 変換後の分散を最大化(互いの相関を小さく?)\n",
    "        - 平均値を引く→共分散行列を計算→固有ベクトルを求める→対応する固有値の大きい順に特徴量を取得する\n",
    "        - 非線形な関係があるときは弱い．それを補うカーネルPCAという手法もある．  \n",
    "        - 分散が大きい方に引っ張られる(変換後の分散を最大にしようとする)ため，分散が小さい方が重要なときクラス分けがしにくい結果になる\n",
    "    - 線形判別分析(Linear Discriminant Analysis: LDA)\n",
    "        - 異なるクラス間のデータポイントの分散を最大化し，クラス内の分散を最小化しようとするため，分散によって引っ張られない．\n",
    "        - ラベルを渡す，「教師あり」の手法である．\n",
    "        - クラス数が増え，クラスあたりのサンプルが少なくなると機能しにくくなっていく\n",
    "    - 多次元尺度構成法(Multi Dimensional Scaling: MDS)（手法の総称)\n",
    "        - 各データポイントを低次元空間に配置しつつ，低次元空間での新しい距離が元の特徴空間上での距離とできるだけ似るようにする\n",
    "        - データの可視化に利用でき，2次元か3次元に変換されることが多い\n",
    "        - データ間の距離を求められるようにしておくことが大事\n",
    "    - 上記の方法がうまくいかなければ多様体学習アルゴリズムにも手を出して見ると良い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12章 ビッグデータ\n",
    "ビッグデータとは，コンピュータが速度やメモリ，データの大きすぎといった問題で処理しきれないデータ  \n",
    "- jug\n",
    "    - 長時間に及ぶ計算において，一時的な結果をキャッシュして置いたり，並列計算させたりできる\n",
    "    - タスク: キャッシュを取られ，並列計算される関数．TaskGeneratorデコレータで指定．依存関係のあるタスクは自動的に解決される\n",
    "    - jug executeで実行され，キャッシュディレクトリが生成される\n",
    "- AWS\n",
    "    - 処理能力の高い環境を一時間単位でレンタルできる\n",
    "    - 大規模なマルチコア，GPUなどを使ってjugなどを利用できる\n",
    "- starcluster\n",
    "    - Amazonのマシン作成作業をスクリプトで自動化するためのパッケージ\n",
    "    - Python2.7までしか対応していない．おそらくPython3ではboto3が推奨\n",
    "    - マスターノードと複数のスレーブノードからなるクラスタを作成できる．\n",
    "    - クラスタ内ではjugなどのジョブをキューイングエンジンによって分担して実行できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 付録 機械学習についてさらに学ぶために\n",
    "- オンラインコース\n",
    "    - Coursera: スタンフォード大学の機械学習などの授業 by Ng\n",
    "        - MOOC (Massive Open Online Course)と呼ばれている\n",
    "- 書籍\n",
    "    - パターン認識と機械学習 by ビショップ : 理論的な側面\n",
    "    - Machine Learning: A Probabilistic Perspective by K. Murphy: 1100p以上の数学重視本\n",
    "- Q&Aサイト\n",
    "    - MetaOptimize, http://metaoptimize.com/qa : 機械学習のためのQAサイト, 有名な研究者，開発者も参加\n",
    "    - Cross Validated http://stats.stackexchange.com : 統計一般のためのQAサイト，機械学習の質問も\n",
    "- ブログ\n",
    "    - 理論寄り\n",
    "        - Machine Learning (Theory): http://hunch.net\n",
    "    - 実用，実践\n",
    "        - Text & data mining by practical means http://textanddatamining.blogspot.de/\n",
    "        - Edwin Chen's blog http://blog.echen.me\n",
    "    - 統計\n",
    "        - FlowingData http://flowingdata.com\n",
    "        - Normal Deviate http://normaldeviate.wordpress.com\n",
    "        - Simply statistics http://simplystatistics.org\n",
    "        - Statistical Modeling, Causal Inference, and Social Science http://andrewgelman.com\n",
    "- データソース\n",
    "    - カリフォルニア大学アーバイン校(UCI)の機械学習リポジトリ http://archive.cs.uci.edu/ml\n",
    "- コンペ\n",
    "    - Kaggle\n",
    "        - ほとんどの勝者は，前処理，正規化，既存手法の組み合わせでやってる\n",
    "- その他の機械学習パッケージ\n",
    "    - Modular toolkit for Data Processing: MDP http://mdp-toolkit.sourceforge.net\n",
    "    - Pybrain http://pybrain.org\n",
    "    - Machine Learning Tooklit: MiLK http://luispedro.org/software/milk\n",
    "        著者の一人によって開発されている\n",
    "- オープンソースの機械学習リポジトリ\n",
    "    - http://mloss.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用した関数のまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用したデータセットのまとめ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
