{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 機械学習は 教師の有無 × 分類OR予測 × 特徴エンジニアリング．そして交差検定\n",
    "1. (教師あり 数値予測) 複雑なモデルは過学習を起こし，逆に単純だと誤差を小さくしきれない\n",
    "2. (教師あり 多値分類) 数値特徴量の集合に対する最初のアプローチは，K近傍法による分類の結果をK分割交差検定で検証\n",
    "3. (教師なし 多値分類) シンプルな文書分類にはbag-of-words ＋ KMeans\n",
    "    - bag-of-words: 出現回数やTF-IDF(単語の貴重さ)を基準にVectorizerで単語をベクトル化．文書をベクトル集合とする．  \n",
    "4. (教師なし 多値分類) 文書データに複数のラベル付けを行いたいときは，トピックモデル\n",
    "    - 潜在的ディリクレ分布(LDA): トピックによる文書生成モデル．オバマとトランプの違いを吸収．wordcloud, 次元削減，類似文書の検索などにもOK．\n",
    "5. (教師あり 二値分類) 分類結果を確率で取得する，ロジスティック回帰による文書の2値分類\n",
    "    - ロジスティック回帰: オッズ比の対数関数の逆関数に入力する線形式の最適化．回帰分析＋シグモイド関数による確率化に相当．係数で次元削減可．\n",
    "    - 二値ラベルが偏ったデータは，何も考えず片方を出すだけで正答率が上がってしまうので，PRCurveのAUC(線下面積)によるモデル評価が有効．\n",
    "        - 適合率(冤罪を起こさない確率)と再現率(犯人を逃さない確率)のトレードオフがなるべく起きないようにする\n",
    "6. (教師あり 二値分類) 特徴量同士が独立に近いならば，ナイーブベイズによる分類も有効\n",
    "    - ナイーブベイズ：そのラベルの付いたデータの割合 × そのラベルのついたデータがその特徴量を持っている割合　が最大のラベルを選択\n",
    "    - GridSearchCVによって最適なハイパーパラメータの組み合わせを求められる．\n",
    "        - 「最適」の指標にはF値(適合率と再現率の調和平均, 逆数の平均，率の平均，低い方に引っ張られる)を使用．\n",
    "7. (教師あり 数値予測) 数値データを推論する最小二乗法は，罰則項で過学習を防げる\n",
    "    - 罰則項： L1(Lasso, 特徴量の絶対値，次元削減)，L2(Ridge, 特徴量の2乗，特徴量をバランスよく)，Elastic Net( αL1+(1-α)L2 )\n",
    "    - 最終的なCV用に訓練データを分離→分離した訓練データからさらに訓練データを分離→その訓練データで対象パラメータの訓練を行う\n",
    "8. (半教師あり 数値予測) 似た商品を購入しているユーザが購入している商品を推薦\n",
    "    - アンサンブル学習: 最小二乗法 ＋ 相関係数　のように複数の特徴量を組み合わせる．係数の低いものは削減できたり，新しい特徴量を得られたり．\n",
    "    - バスケット分析: 「この商品を買った人はこんな商品も買っています」\n",
    "        - 頻出アイテム集合の列挙: アプリオリアルゴリズム(1つの商品からはじめ，別の商品を追加しても頻出する組み合わせを列挙)\n",
    "        - X買ったらYも買う，アソシエーションルールのマイニング: LIFT値(誰もが買っている商品に対するペナルティを課す)\n",
    "9. (教師あり 多値分類 特徴エンジニアリング) 音楽の分類にはFFTより，専門家の設計したより良い特徴量を使う．\n",
    "    - 多値分類問題は1対多の二値分類問題に変換できる．\n",
    "    - ROC曲線のAUCによる評価: TP rate VS FP rate, 別々のラベル内での正解率を指標とするので偏りに強いが，PR曲線よりは鈍感\n",
    "    - メル周波数ケプストラム係数(MFCC): 波形→離散フーリエ変換→対数を取る→メルフィルタバンクをかける→離散コサイン変換→低次の係数を取得\n",
    "10. (教師あり 多値分類 特徴エンジニアリング) 画像データの特徴量\n",
    "    - テクスチャベースの特徴量: 明暗の差による画像の分類\n",
    "        - Haralickテクスチャ特徴量，Linear Binary Patterns特徴量(線形演算を行なっても不変なので，照明の変化にロバスト) など\n",
    "        - ソーベルフィルタによる特徴量: 水平方向のフィルタと垂直方向のフィルタでエッジ検出を行い，それらの内積を，画像の「エッジさ」とする．\n",
    "    - 局所特徴量による記述子: 画像の局所的な領域を対象に計算される特徴ベクトル\n",
    "        - SURF(Speeded Up Robust Features), SIFT(Scale-Invariant FeatureTransform) など\n",
    "            - これらで得られた記述子をクラスタリングにかけることで，似たような音声を同じ単語と認識するように画像のクラス分けができる\n",
    "11. (特徴エンジニアリング) 不必要な特徴量を削除することで，テスト精度向上，高速化，可視化などのメリットが得られる．  \n",
    "    - フィルター法による特徴選択: 相関係数(線形な関係性のみ) や 相互情報量(片方が分かったときもう片方がどれだけ分かるか) の高い特徴量を削除\n",
    "    - ラッパー法による特徴選択: RFE(再帰的特徴削減)など，分類器をラッピングし，訓練しながら影響の小さな特徴量を削除していく\n",
    "    - 特徴抽出: 特徴選択よりさらに次元を削減したい場合や，可視化したい場合に行う\n",
    "        - 主成分分析(PCA): 共分散行列の固有ベクトルに対応する固有値の大きい順に取得，非線形関係や小分散データが重要な時は弱い\n",
    "        - 線形判別分析(LDA): クラス内の分散を最小化する教師ありの手法．クラス数が多く，クラス当たりのサンプルが少ないと弱い\n",
    "        - 多次元尺度構成法(MDS): 変換後の低次元空間での距離が元の特徴空間上での距離とできるだけ似るようにする．可視化に使える．\n",
    "12. (計算の高速化，効率化) ビッグデータとは，コンピュータが処理しきれないデータ\n",
    "    - jug: 関数をタスク化することで，一時的な結果をキャッシュして置いたり，並列計算させたりできるパッケージ\n",
    "    - AWS: 処理能力の高い環境を時間単位でレンタルできる．starclusterやboto3によるスクリプトで，jugと合わせてマシンを並列起動させられる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1章 Pythonではじめる機械学習\n",
    "\n",
    "- データを理解し，データを扱いやすい形に整形することは大事\n",
    "- アルゴリズムの出した結果に対して，正しい評価を行うことが大事\n",
    "    - 訓練データとテストデータを区別したり\n",
    "- モデルとは，複雑な現実世界で起こる現象を理論的に単純化して近似したもの\n",
    "    - モデルと実際のデータの誤差を小さくし，かつ未知のデータにも正しい答えを出力できるようにするのが目的\n",
    "- 多項式フィッティングを使ってWebトラフィックの予測を行なった．\n",
    "    - 次数を大きくしすぎると過学習を起こす\n",
    "    - データの特徴から，フィッティング曲線を時間毎に分けてみることも一つの手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2章 実例を対象とした分類法入門\n",
    "クラス分類の基本的な話\n",
    "- データセット\n",
    "    - Irisデータセット： 小規模で広く使われている\n",
    "    - Seedsデータセット： 小麦の産地を7つの特徴量から推定する\n",
    "- 分類モデル\n",
    "    - 閾値を使ったモデル: 人間の知識やデータの考察が必要\n",
    "    - 最近傍法: 訓練特徴量をベクトルとし，もっとも近いN次元上の点のラベルを正解とする手法\n",
    "    - K近傍法: もっとも近いK個の点のラベルの多数決の結果を正解とする手法\n",
    "- 評価手法\n",
    "    - 訓練データをモデルの評価に使ってはいけない\n",
    "    - 交差検定\n",
    "        - leave-one-out: データの一つを取り除いてそれ以外のデータで訓練した結果，取り除いたデータに対し正しい結果を返すかを確かめる\n",
    "        - K分割交差検定: データをK個に分割し，それぞれのブロックを取り除いて訓練した結果，取り除いたブロックに対しどれだけ正しい結果を返すかを確かめる\n",
    "- 特徴量について\n",
    "    - 洗練されたアルゴリズムより良いデータ，良い特徴量\n",
    "    - 特徴選択： 良く分類ができる特徴量を選択する手法\n",
    "    - 特徴エンジニアリング: 良く分類ができるような特徴量を，現存する特徴量から計算で作り出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3章 クラスタリング 関連のある文書を見つける\n",
    "\n",
    "関連する文書を素早く見つけるため，文書のクラスタリング(教師なし学習)を行った．\n",
    "\n",
    "- 文書の前処理\n",
    "    - bag-of-words化: 単語の出現回数を数え，その文書の特徴とする，Scikit-Learnでできる\n",
    "        - 同じ文章が繰り返されているだけで単語の重要度が上がってしまうので，出現回数の正規化を行う\n",
    "        - min_df, max_df: 滅多に出現しない単語，非常に多く一般的に使われる単語を取り除く\n",
    "        - stop words: その言語体系で文書の分類に貢献しないような一般的な単語を取り除く\n",
    "        - ステミング: 語幹を数える (ex. imaging -> image)\n",
    "        - TF-IDF: その単語がどれだけすべての文書の中で特徴的かを表す指標，Scikit-Learnで算出できる\n",
    "    - N-gram: not eatのように否定を表したりすることを認識させたいならば，前後するペア(バイグラム)や3つの単語(トリグラム)を利用する\n",
    "- クラスタリング手法\n",
    "    - 新しい文書を分類するとき，bag-of-wordsのノルムを見れば良いだけになり，素早い分類が可能になる\n",
    "    - フラットクラスタリング: 全てのデータが高々１つのクラスタに所属\n",
    "        - KMeans: 広く使われるクラスタリング手法\n",
    "            1. クラスタの数num_clustersを指定して初期化\n",
    "            1. num_clustersの数だけランダムにデータを選び出し，それらの特徴ベクトルを各クラスタの中心点とする\n",
    "            1. その他のデータは，もっとも近い中心点を持つクラスタに所属させる\n",
    "            1. 全てのデータのクラスタが決まったら，各クラスタの中心を新たに計算し，そこに中心点を移動させる\n",
    "            1. 中心点の移動量が閾値を下回る(収束)，または指定の繰り返し回数に達するまで2~4を繰り返す\n",
    "    - 階層的クラスタリング: クラスタのクラスタ，のように階層構造を持つ\n",
    "    - コサイン類似度，ピアソンの相関係数，Jaccard係数などの他の類似度の指標を使ったりしてマイナーチェンジしてみてもいい\n",
    "    - 良いクラスタリングの指標を明確にするのが大事．代表的な指標はsklearn.metrixにある．\n",
    "\n",
    "文書のクラスタリングをするときの流れ\n",
    "1. 文書データを集める\n",
    "1. 文書データをScikit-LearnでVectorizeする\n",
    "1. Vectorizeされた文書をScikit-LearnのKMeansなどでクラスタリング\n",
    "1. 新しい文書データを分類する\n",
    "    1. 新しい文書データをVectorizeする\n",
    "    1. クラスタリング結果を持つKMeansに文書をPredictさせると，新しい文書データの所属するクラスタが素早くわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4章 トピックモデル\n",
    "トピックモデル: データの一つ一つを「複数の」グループに割り当てる問題を扱う分野．で，文書データを分類\n",
    "- LDA(潜在的ディリクレ配分法, Latent Dirichlet Allocation): 最も単純なトピックモデル\n",
    "    - テキスト分類を考える．\n",
    "        - すべての単語に，関連するトピックを割り当てる．\n",
    "        - トピックの正体は単語についての多項分布で，各単語とそのトピックとの関連の深さを確率で表現している．\n",
    "        - ドキュメントに入っている単語のトピック強度の総和がそのドキュメントのトピック強度になる．  \n",
    "        - LDAは文章生成モデルで，あるトピックを選ぶとそのトピックのドキュメントが最もよく生成されるようにフィッティングする．\n",
    "    - 例えば，オバマと書いてあるドキュメントとトランプと書いてあるドキュメントを，トピックが似ている，と分析できる．\n",
    "    - gensimパッケージでLDAを使える．\n",
    "        - コーパスとid2wordを与えてやると学習完了．モデルにドキュメントを与えると関連するトピックが返ってくる．\n",
    "        - alphaで一つのドキュメントに割り当たるトピックの数が変わる．\n",
    "        - あるトピックが何のトピックであるのかは，そのトピックのトピック強度が強い単語を見て考えてやる．\n",
    "        - 前処理(ステミングや無関係文字の削除)を行うとなおよい\n",
    "        - alphaやトピック数といったパラメータを変えても，最終的な結果にはあまり影響がない．\n",
    "    - 学習後のモデルそのままでもwordcloudによる可視化などで役に立つ．\n",
    "    - 次元削減の効果があり，他の手法の中間的な役割を担うこともできる．\n",
    "    - 各ドキュメントのすべてのトピック強度をベクトルとすると，最もトピックが近い文章を求めることができる．\n",
    "- 階層ディリクレ過程(hierarchical Dirichlet process: HDP): トピック数をデータセットに応じて自動決定するトピックモデル\n",
    "    - 直観的には，文章が多ければ多いほど多くのトピックを得ることを利用する．\n",
    "    - 例えば，ニュースの記事データが少なければトピックは「スポーツ」になるかもしれないが，多ければ「サッカー」などと細かくできる．\n",
    "    - この手法はgensimに用意されており，先ほどのLDAをHDPに変えるだけで実現できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 クラス分類 悪い回答を判別する\n",
    "\n",
    "ノイズの多いテキストデータを良い回答，悪い回答に分類するにはどうすれば良いか？という問題に取り組んだ．  \n",
    "使用したデータはstackoverflowの問答データ．教師データになりそうなのはScore．  \n",
    "以降の分類器はscikit-learnに便利な関数が多数用意されている．  \n",
    "\n",
    "- K近傍法による分類\n",
    "    - 特徴量を増やしたり，Kの値を大きくしてモデルを単純にしても，時間がかかるだけで精度が上がらないことがある．  \n",
    "    - モデルを単純にしても特徴量を改良してもバリアンスが高いので，K近傍法はこのタスクに向かないとわかった．\n",
    "        - バイアス-バリアンストレードオフ\n",
    "            - (エラーの)バイアスが高い->未学習, 訓練誤差もテスト誤差も大きい, データのノイズが多い， 特徴量の設計がマズい\n",
    "            - バリアンスが高い->過学習, 訓練誤差とテスト誤差の差が大きい, モデルが複雑すぎる\n",
    "            - これらはトレードオフであり，最適解を探したい\n",
    "- ロジスティック回帰による分類\n",
    "    - ロジスティック回帰: どんな入力にも白か黒かの確率を返す関数の回帰による最適化\n",
    "        - 要は「オッズ比の対数関数の逆関数に入力する線形式の最適化」\n",
    "        - オッズ比：$P/(1-P)$\n",
    "        - 入力無限領域，出力0~1の確率の形にできる\n",
    "        - 1次式を入力するシグモイド関数とも言える\n",
    "        - バイアス+ 係数 ＊ 特徴量の値 + 係数 ＊ 特徴量の値 + ... をロジスティック回帰の式に入力することでクラスに属する確率を得る\n",
    "        - 正規化パラメータCがある\n",
    "    - 最適なCを選んでも，90NNと同じくらいの精度しか出ない．\n",
    "        - バイアスが大きい\n",
    "    - 悪い回答の確率と良い回答の確率をそれぞれ求めるより，片方を求めてもう片方はあまり，としたときの精度の方が役に立つのでは？\n",
    "        - 適合率-再現率曲線(Precision-Recall curve)のAUC(Area Under Curve)がもっとも大きい閾値を求める\n",
    "            - 適合率： $TP / (TP + FP)$ 冤罪を起こさない確率\n",
    "            - 再現率: $ TP / (TP + FN) $ 犯人を逃さない確率\n",
    "            - 適合率と再現率はトレードオフ． 再現率をあげてもどれだけ適合率が下がらないかをグラフでみるのが適合率-再現率曲線\n",
    "    - 結果として，良い回答かどうかを分類する方が，悪い回答かどうかを分類するより高いAUCを出せることがわかった．  \n",
    "    - ロジスティック係数が0に近い特徴量は分類器の精度にあまり影響を与えないので省ける\n",
    "    - このモデルをで保存しておくことで，Webサービスなどでは素早く回答の良し悪しを判別させられる．\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6章 クラス分類Ⅱ 感情分析\n",
    "ツイートのポジティブ/ネガティブ分類器を作る．  \n",
    "- ナイーブベイズ\n",
    "    - 分類したいクラスを$C$,特徴量を$F_1, F_2$とするとベイズの定理から$$ P(C | F_1, F_2) = \\frac{P(C) \\cdot P(F_1, F_2 | C)}{P(F_1, F_2)} $$\n",
    "    - $P(C | F_1, F_2)$: 事後確率 $F_1$と$F_2$がわかっているとき，そのデータがクラス$C$に属する確率\n",
    "    - $P(C)$: 事前確率, データについて何も情報がない場合にそのデータがクラス$C$に属する確率\n",
    "    - $P(F_1, F_2 | C)$: 尤度, あるデータがクラス$C$に属することがわかっている場合，特徴量がその$F_1，F_2$である確率\n",
    "    - $P(F_1, F_2)$: 証拠, 特徴量がその$F_1$と$F_2$をとる確率, 該当する特徴量が全体に占める割合を計算して求める\n",
    "    - これよりクラスの判定結果は$$ C_{best} = \\arg \\max_{c \\in C} P(C=c) \\cdot P(F_1 | C=c) \\cdot P(F_2 | C=c) $$\n",
    "    - 新出データに対して確率を求めると証拠が0になってしまうので，全ての単語が1度は出ていることを仮定するスムージングを行う．\n",
    "        - 加算スムージング(ラプラススムージング): 全ての頻度に1を足す\n",
    "        - Lidstoneスムージング: 0以上のパラメータ$\\alpha$を足す\n",
    "    - 実用上は，限りなく小さい確率の積によるアンダーフローを防ぐため，積を和に変換できる対数をとる．この場合，\n",
    "    $$ C_{best} = \\arg \\max_{c \\in C} \\log P(C=c) + \\sum_k \\log P(F_k | C=c) $$\n",
    "    - sklearnには3つのナイーブベイズ分類器が用意されている． 今回はMultinomialNBを利用．\n",
    "        - GaussianNB: 特徴量が正規分布(ガウス分布)にしたがって分布すると仮定, 身長と体重から性別を分類する問題など\n",
    "        - MultinomialNB: ある事象が発生した回数を特徴量としていると仮定, TF-IDFと相性がよい\n",
    "        - BernoulliNB: 単語が出現した / 出現していない の2値で特徴量が表される場合に適している．  \n",
    "<br>\n",
    "- ポジティブとネガティブを判定する分類器\n",
    "    - TfidfVectorizer->MultinomialNB というモデルを Pipelineで繋いだモデルで分類．\n",
    "    - 以降，データが少ないので，交差検定を行う．ShuffleSplitでテストデータと訓練データを分ける．\n",
    "    - 正解率80%, AUC(Area Under precision-recall curve)88%(以下80/88のように表記)の良い感じの結果に．  \n",
    "<br>\n",
    "- 全てのクラスを判定する分類器\n",
    "    - 感情を含む / 含まない: 79/67: 感情を含むツイートは2割なので役立たず\n",
    "    - ポジティブ/ その他: 90/27: PRカーブの成績が悪い\n",
    "    - ネガティブ/ その他: 88/47: PRカーブの成績が悪い\n",
    "    - データに偏りがあるのでPRカーブを信じるべき．するとこの結果はあまり良くない．  \n",
    "<br>\n",
    "- 感情を含む / 含まないの判定についてパラメータ最適化を行なった分類器\n",
    "    - GridSearchCVによって交差検定(Cross Validation)の結果が最も良いパラメータを全探索\n",
    "        - Param_grid = {\"vect__ngram_range\": [(1,1), (1,2), (1,3)]} のような辞書を作る．\n",
    "    - 交差検定の指標にはF値(metrics.f1_scoreで実装)を用いる．$$ F = \\frac{2 \\cdot precision \\cdot recall}{precision + recall} $$\n",
    "        - precisionとrecallの調和平均(逆数の平均の逆数,率の平均を取るのに適する,値の小さい方に引っ張られる平均)\n",
    "    - 感情を含む / 含まない: 83/70 ちょっと良くなった  \n",
    "<br>\n",
    "- ツイートの整形を行う分類器(パラメータ最適)\n",
    "    - TfidfVectorizerにpreprocessorを渡す．\n",
    "        - 顔文字を単語に変換\n",
    "        - 短縮系の言葉を単語に分割\n",
    "    - pos vs neg: 80/88, emo vs rest: 83/70, pos vs rest: 91/51, neg vs rest: 90/65  \n",
    "<br>\n",
    "- 単語のポジネガスコアを利用する分類器(パラメータ最適，整形済み)\n",
    "    - nltk.pos_tag(nltk.word_tokenize(\"This is a sentense.\")) による単語品詞タグづけ(Penn Treebankプロジェクト)\n",
    "    - SentiWordNetのデータによる「品詞/単語」のポジネガスコアを獲得し，特徴量とする\n",
    "    - BaseEstimatorを継承して，ポジネガスコアを特徴量として算出するクラスを作り，TfidfVectorizerの特徴量とFeatureUnionによって合わせる．\n",
    "    - pos vs neg: 80/88, emo vs rest:83/69, pos vs rest: 91/52, neg vs rest: 89/62, 努力の割にはそんなに変わらず\n",
    "    - まず感情があるかないかを分類してからポジネガ分類器にかけると良いかもしれない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7章 レコメンド\n",
    "\n",
    "- 最小二乗法の復習\n",
    "    - sklearnのボストン物件価格データを利用\n",
    "    - 特徴量一つだけよりバイアスを加えた方が，さらにたくさん特徴量があった方が一般的に良い精度が出る．\n",
    "    - 回帰分析でも交差検定をして正しい汎化能力を確かめよう．\n",
    "    - 特徴量の数P > サンプル数N のとき，訓練誤差をほぼゼロにすることができるが，汎化能力が著しく落ちる．\n",
    "        - 10-K reportsのデータはP>Nとなっており，これでP>N問題を確かめた．\n",
    "    - 過学習対策\n",
    "        - Lasso: L1罰則項(回帰係数の絶対値の総和)をペナルティとする．\n",
    "            - スパースなモデルに．\n",
    "        - Ridge: L2罰則項(回帰係数の2乗の総和)をペナルティとする．\n",
    "        - Elastic Net: L1, L2の比率を設定し，それらの和を罰則項とする． \n",
    "            - P>N問題の解消，相関の高い特徴量からの複数考慮といった効果がある．\n",
    "- パラメータ探索には2段階の交差検定が必要\n",
    "    - 1段目： テストデータと訓練データに分ける\n",
    "    - 2段目： 1段目の訓練データをさらに分割し，それぞれ違ったパラメータを与えて訓練した結果，誤差が最少となったパラメータを採用\n",
    "- 映画の推薦問題を罰則付き回帰分析で解いてみる\n",
    "    - Netflix Challengeのアカデミック用データを利用\n",
    "    - ターゲットユーザ以外のユーザが行なった評価を特徴量としてLassoで解いてみた\n",
    "    - 全体の平均をスコアの予測値とするよりはLassoを使った方が8％ほどの精度改善が見られた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8章 レコメンドの改良\n",
    "- 映画の推薦システムの改良\n",
    "    - スコアではなく，どの映画にスコアをつけたかに注目\n",
    "        - スコアをつけた映画が似ているユーザのスコアを予測値とする手法\n",
    "            - 各ユーザの相関係数を利用する\n",
    "        - 逆に，スコアをつけたユーザが似ている映画に着目してみる手法\n",
    "    - アンサンブル学習: 上記の2つの手法のような，複数の手法を組み合わせて予測を行う手法\n",
    "        - 各手法の予測結果は新たな特徴量と考えられるので，回帰によって重み付けを行える．\n",
    "            - 重み付けの結果，重みの小さな特徴量は役に立たず，全体のパフォーマンスを下げる可能性もある．\n",
    "            - 複数の手法を簡単に比較検討する手段としても使える．\n",
    "- バスケット分析: 「この商品を買った人はこんな商品も買っています」\n",
    "    - どの商品が一緒に買われているかに着目する．\n",
    "        - あるユーザが複数の商品を買っている時，そのユーザが他にどんなものを買う傾向があるのかを予測できる．  \n",
    "        - 生活必需品など，多くの人が買わざるを得ない商品はユーザの趣味嗜好に関係がないので注意する．\n",
    "    - ベルギーのスーパーマーケットでの匿名トランザクションデータ, retail.datを使って実験\n",
    "    - 1. 頻出アイテム集合の列挙\n",
    "        - アプリオリアルゴリズム: 頻出アイテム集合: 頻繁に購入される商品の組み合わせ をトランザクション集合から列挙する．\n",
    "            - 購入頻度(支持度)が閾値(最小支持度)以上の商品1つによる集合を頻出アイテム集合とするところから始める\n",
    "            - 頻出アイテム集合に一つ頻出アイテムを追加し，その支持度を求め, それがまだ最小支持度以上ならば頻出アイテム集合として記録していく\n",
    "            - 新たな頻出アイテム集合がどのトランザクションに含まれているかをキャッシュすることによって高速化することができる．\n",
    "    - 2. アソシエーションルールマイニング\n",
    "        - 頻出アイテム集合の列挙を元に，アソシエーションルールの強さを導く\n",
    "            - アソシエーションルール: 「この商品の組みあわせXを買っている人はこの商品Yを買っている傾向がある」\n",
    "        - アソシエーションルールの強さの指標として，次のLIFT値を用いる. 誰もが買っている商品に対するペナルティを課している．\n",
    "            $$ lift(X \\Rightarrow Y) = \\frac{P(Y | X)}{P(Y)} $$\n",
    "        - 1で作成したキャッシュを利用することで高速化できる．\n",
    "    - 他にも，買い物の順序を考慮した手法などが，pyminingというパッケージで提供されている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9章 クラス分類Ⅲ 音楽ジャンル分類\n",
    "音楽の複数ジャンル分類問題\n",
    "- GTZANというデータセットを利用\n",
    "    - 10個のジャンルのうち6個を今回は使用\n",
    "- FFTによる分類: あまりうまくいかない\n",
    "    - FFT, 高速フーリエ変換: 波形データを周波数の違うサイン波に分解し，周波数ごとの強度を可視化する\n",
    "        - バタフライ演算による対数オーダーの高速化\n",
    "    - ロジスティック回帰によるジャンル分類\n",
    "        - 多値分類問題にも適用できるが，2値分類問題として学習させた方がよさげ\n",
    "    - 混同行列: どのジャンルが正解のとき，どのジャンルに何曲分類されたかを可視化できる．対各成分が濃いと良い．\n",
    "        - matplotlibのmatshow関数で可視化できる\n",
    "    - ROC(receiver operator characteristic)曲線による評価\n",
    "        - false positive rateとtrue positive rateによる曲線，こちらもP/R曲線と同様，AUCが高いほど良い\n",
    "        - P/R曲線は陽性サンプルが貴重なとき有効なのに対し，ROCは分類器の振る舞いについて全体像を見たいときに利用する．\n",
    "- メル周波数ケプストラム係数(MFCC)による分類: 割とうまくいく\n",
    "    - MFCC\n",
    "        - 音声の波形データを低次元に落とし込む「手法」\n",
    "        - 波形→離散フーリエ変換→対数を取る→メルフィルタバンクをかける→離散コサイン変換→低次の係数を取得\n",
    "        - libsoraパッケージで波形からMFCCを得る関数を利用可能\n",
    "    - ROC曲線のAUC,混同行列の対角線の濃さ共に全体的に良好だが，カントリーやジャズの分類は難しい模様"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10章 コンピュータビジョン パターン認識\n",
    "- 基本的な画像処理 by mahotas\n",
    "    - 閾値処理(二値化)\n",
    "        - 大津の二値化: 高めの閾値，大まかな分離をさせたい時に有効\n",
    "        - Ribler-Calvardの二値化: 低めの閾値，二値化しても細部を残したい時に有効\n",
    "        - open, closeでノイズ除去\n",
    "    - ガウシアンぼかし\n",
    "        - 標準偏差を大きくするほど強くボケるフィルタ\n",
    "        - ぼかしてから二値化すると大きめのノイズも除去できる\n",
    "    - フィルタ処理\n",
    "        - ソルト&ペッパーノイズ: スキャナーのシミュレーションやロバストなデータ作りに\n",
    "        - 周囲をぼかす： ガウス分布で中心から外側に行くほどガウシアンぼかしを行なった画像の要素を強くする\n",
    "- シンプルなデータセットのパターン認識\n",
    "    - パターン認識: 画像のクラス分類\n",
    "    - Haralickテクスチャ特徴量: 風景のような滑らかな画像とテキストのような明暗がはっきりした画像の分類\n",
    "        - ロジスティック回帰で72%\n",
    "    - LInear Binary Patterns特徴量: 線形演算を行なっても普遍なので照明の変化にロバスト\n",
    "    - ソーベルフィルタを使った特徴量:\n",
    "        - 水平方向のフィルタと垂直方向のフィルタでエッジ検出を行う．\n",
    "        - 検出されたエッジの内積による数値化によって大域特徴量にする．\n",
    "        - Haralickと合わせて84%\n",
    "- 難しいデータセットのパターン認識\n",
    "    - テクスチャ特徴量だと52%\n",
    "    - 局所特徴量による記述\n",
    "        - 画像の局所的な領域を対象に計算される特徴量\n",
    "        - ランダム， グリッド， 特徴領域検出といった手法で特徴量を得る領域を指定\n",
    "        - SURF(Speeded Up Robust Features), SIFT(Scale-Invariant FeatureTransform)など\n",
    "            - descriptor(記述子, 画像中の対象領域を特徴ベクトルに変換した結果)を得る\n",
    "        - descriptorにbag-of-wordsを適用する\n",
    "            - descriptorをクラスタリングにかけることで，似たような音声を同じ単語と認識するように，画像のクラス分けができる\n",
    "            - クラスター，画像の単語の数のようなものは，極端に大きかったり，小さかったりしないのが望ましい．\n",
    "        - SURF: 61%, SURF+sobel+haralick: 65%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11章 次元削減\n",
    "不必要な特徴量を削除することで，テスト精度向上，高速化，可視化などのメリットが得られる．  \n",
    "\n",
    "- 特徴選択\n",
    "    - フィルター法: 関連性の強い特徴量を削除\n",
    "        - ピアソンの相関係数を関連性の指標とする\n",
    "            - 線形な関連性しか見れない\n",
    "            - P値： その相関係数が偶然である確率，信頼性の低さ\n",
    "        - 相互情報量を関連性の指標とする\n",
    "            - 相互情報量： 片方がわかったとき，もう片方のエントロピー(不確実性)をどれだけ減らすか→共依存度\n",
    "            - 非線形な関連性も見れる\n",
    "        - 特徴量の全てのペアの組み合わせを見ていると大変時間がかかる(2乗オーダ)\n",
    "    - ラッパー法: 分類器をラッピングし，訓練しながら影響の小さな特徴量を削除していく\n",
    "        - モデルの視点からは不必要な特徴量がまだあったり，複数の特徴量を組み合わせることで有用になることがあることに着目\n",
    "        - RFE(Recursive Feature Elimination, 再帰的特徴削減)が主要な手法\n",
    "    - 決定木やL1ノルムのように，特徴選択は学習プロセスの中に含まれていることがある．\n",
    "- 特徴抽出\n",
    "    - 特徴選択よりさらに削減したい場合や，可視化したい場合に行う\n",
    "    - 主成分分析(Principal Component Analysis: PCA) まずはコレ！\n",
    "        - 高速で逆変換可能(逆変換時の誤差を最小にする）\n",
    "        - 変換後の分散を最大化(互いの相関を小さく?)\n",
    "        - 平均値を引く→共分散行列を計算→固有ベクトルを求める→対応する固有値の大きい順に特徴量を取得する\n",
    "        - 非線形な関係があるときは弱い．それを補うカーネルPCAという手法もある．  \n",
    "        - 分散が大きい方に引っ張られる(変換後の分散を最大にしようとする)ため，分散が小さい方が重要なときクラス分けがしにくい結果になる\n",
    "    - 線形判別分析(Linear Discriminant Analysis: LDA)\n",
    "        - 異なるクラス間のデータポイントの分散を最大化し，クラス内の分散を最小化しようとするため，分散によって引っ張られない．\n",
    "        - ラベルを渡す，「教師あり」の手法である．\n",
    "        - クラス数が増え，クラスあたりのサンプルが少なくなると機能しにくくなっていく\n",
    "    - 多次元尺度構成法(Multi Dimensional Scaling: MDS)（手法の総称)\n",
    "        - 各データポイントを低次元空間に配置しつつ，低次元空間での新しい距離が元の特徴空間上での距離とできるだけ似るようにする\n",
    "        - データの可視化に利用でき，2次元か3次元に変換されることが多い\n",
    "        - データ間の距離を求められるようにしておくことが大事\n",
    "    - 上記の方法がうまくいかなければ多様体学習アルゴリズムにも手を出して見ると良い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12章 ビッグデータ\n",
    "ビッグデータとは，コンピュータが速度やメモリ，データの大きすぎといった問題で処理しきれないデータ  \n",
    "- jug\n",
    "    - 長時間に及ぶ計算において，一時的な結果をキャッシュして置いたり，並列計算させたりできる\n",
    "    - タスク: キャッシュを取られ，並列計算される関数．TaskGeneratorデコレータで指定．依存関係のあるタスクは自動的に解決される\n",
    "    - jug executeで実行され，キャッシュディレクトリが生成される\n",
    "- AWS\n",
    "    - 処理能力の高い環境を一時間単位でレンタルできる\n",
    "    - 大規模なマルチコア，GPUなどを使ってjugなどを利用できる\n",
    "- starcluster\n",
    "    - Amazonのマシン作成作業をスクリプトで自動化するためのパッケージ\n",
    "    - Python2.7までしか対応していない．おそらくPython3ではboto3が推奨\n",
    "    - マスターノードと複数のスレーブノードからなるクラスタを作成できる．\n",
    "    - クラスタ内ではjugなどのジョブをキューイングエンジンによって分担して実行できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 付録 機械学習についてさらに学ぶために\n",
    "- オンラインコース\n",
    "    - Coursera: スタンフォード大学の機械学習などの授業 by Ng\n",
    "        - MOOC (Massive Open Online Course)と呼ばれている\n",
    "- 書籍\n",
    "    - パターン認識と機械学習 by ビショップ : 理論的な側面\n",
    "    - Machine Learning: A Probabilistic Perspective by K. Murphy: 1100p以上の数学重視本\n",
    "- Q&Aサイト\n",
    "    - MetaOptimize, http://metaoptimize.com/qa : 機械学習のためのQAサイト, 有名な研究者，開発者も参加\n",
    "    - Cross Validated http://stats.stackexchange.com : 統計一般のためのQAサイト，機械学習の質問も\n",
    "- ブログ\n",
    "    - 理論寄り\n",
    "        - Machine Learning (Theory): http://hunch.net\n",
    "    - 実用，実践\n",
    "        - Text & data mining by practical means http://textanddatamining.blogspot.de/\n",
    "        - Edwin Chen's blog http://blog.echen.me\n",
    "    - 統計\n",
    "        - FlowingData http://flowingdata.com\n",
    "        - Normal Deviate http://normaldeviate.wordpress.com\n",
    "        - Simply statistics http://simplystatistics.org\n",
    "        - Statistical Modeling, Causal Inference, and Social Science http://andrewgelman.com\n",
    "- データソース\n",
    "    - カリフォルニア大学アーバイン校(UCI)の機械学習リポジトリ http://archive.cs.uci.edu/ml\n",
    "- コンペ\n",
    "    - Kaggle\n",
    "        - ほとんどの勝者は，前処理，正規化，既存手法の組み合わせでやってる\n",
    "- その他の機械学習パッケージ\n",
    "    - Modular toolkit for Data Processing: MDP http://mdp-toolkit.sourceforge.net\n",
    "    - Pybrain http://pybrain.org\n",
    "    - Machine Learning Tooklit: MiLK http://luispedro.org/software/milk\n",
    "        著者の一人によって開発されている\n",
    "- オープンソースの機械学習リポジトリ\n",
    "    - http://mloss.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用した関数のまとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 1章 多項式による数値予測\n",
    "\n",
    "import scipy as sp\n",
    "data = sp.genfromtxt(\"ch01/data/web_traffic.tsv\", delimiter=\"\\t\") # scipyでデータ読み込み\n",
    "fp1, residuals, rank, singular_value, relative_condition_number = sp.polyfit(x, y, deg=1, full=True) # 近似モデルのパラメータを取得\n",
    "f1 = sp.poly1d(fp1) # モデルパラメータからモデル関数を作成\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "fsolve(f1, 800) # x = 800に最も近いモデル関数の解を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 2章 アイリスデータセットの最近傍法，K近傍法による分類\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris() # アイリスデータセットを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 3章 bag-of-wordsによる文書のKMeansクラスタリング\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer(min_df=word_count_to_ignore, stop_words='english') # 単語の出現個数による文書のベクトル化クラス\n",
    "X = vectorizer.fit_transform([\"How to format my hard disk\", \" Hard disk format problems \"]) # 文書のベクトル表現を取得(疎行列形式)\n",
    "vectorizer.get_feature_names() # どの次元がどの単語に対応しているかを取得\n",
    "vectorizer.transform(new_post) # vectorizerに学習されているモデルで，新しい文書をベクトル化\n",
    "\n",
    "vectorizer.get_stop_words() # 一般的すぎる無駄な単語の一覧\n",
    "analyzer = vectorizer.build_analyzer(); analyzer(post) # ドキュメントの前処理を行う\n",
    "tfidf_vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]] # tfidfのidfの部分を計算\n",
    "\n",
    "\n",
    "import scipy as sp\n",
    "sp.linalg.norm(array) # ユークリッド距離の計算\n",
    "\n",
    "import sys\n",
    "sys.maxsize # 最大値\n",
    "\n",
    "import nltk.stem\n",
    "nltk.download('stopwords', download_dir=\"./data/\") # nltkの処理に必要なデータのダウンロード\n",
    "nltk.data.path.append(\"./data\") # downloadで取得したデータのパスをnltkに教える\n",
    "stemmer = nltk.stem.SnowballStemmer('english') # ステミング(語幹を取得する)ためのクラス\n",
    "stemmer.stem(word) # ステミングを行う\n",
    "\n",
    "from scipy.stats import norm\n",
    "x = norm(loc=0.3, scale=.15).rvs(20) # NORMal distributionからRandom VariableS 20個を取得\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(init='random', n_clusters=3, verbose=1, n_init=1, max_iter=1, random_state=seed_to_fix) # KMeans法のインスタンス\n",
    "km.fit(x, y) # クラスタリングを行う\n",
    "km.predict(x_for_prediction, y_for_prediction) # 与えられたデータがどのクラスタに属するかを取得\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(data_home=data_home, categories=categories) # 20newsデータを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 4章 トピックモデルによる文書分類\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "corpus = corpora.BleiCorpus('./data/ch04/ap/ap.dat', './data/ch04/ap/vocab.txt') # テキストデータからBleiのLDA-C形式によるコーパスを生成\n",
    "corpora.textcorpus.TextCorpus # gensimで使うコーパスクラス\n",
    "corpus.get_texts() # コーパスのテキストを取得\n",
    "corpus.dictionary # コーパスのid2word\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=corpus.id2word, alpha=control_tp_num_for_doc) # コーパスからLDAモデルを取得\n",
    "model.id2word[topic_id] # トピックに対応する単語を取得\n",
    "\n",
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_output_wordids.txt')\n",
    "mm = gensim.corpora.MmCorpus('wiki_en_output_tfidf.mm') # 疎な行列のフォーマットにMatrix Marketというものがある．  \n",
    "model.save('wiki.lda.pkl') # 時間がかかる作業は結果を保存しておく\n",
    "model = gensim.models.ldamodel.LdaModel.load('wiki_lda.pkl') # ロードはこう\n",
    "model.show_topic(topic_id, 64) # 与えられたドキュメントに強く関連するトピック64個に対応する単語\n",
    "\n",
    "hdp = models.hdpmodel.HdpModel(mm, id2word) # トピックの数を自動で設定する階層ディリクレ過程(Hierarchical Dirichlet Process) のクラス\n",
    "\n",
    "import wordcloud # というモジュールがある\n",
    "\n",
    "from collections import defaultdict\n",
    "defaultdict(int) # 辞書に入っていないキーにアクセスされたときのデフォルト値を返す関数を設定できる辞書． この場合はintの返り値0になる\n",
    "\n",
    "from scipy.spatial import distance\n",
    "distances = distance.pdist(data_points) # 全てのドキュメントの組み合わせの距離を求める．pairwise distances list. 自分自身との距離はない\n",
    "distances = distance.squareform(distances) # 距離のリストを2次元の距離行列に変換する．対各成分はこのとき0なので必要なら後から大きな値にする\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO) # ログの出力設定\n",
    "logging.info(\"message\") # ログを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 5章 ロジスティック回帰の結果をKFold＋PR曲線でテスト\n",
    "\n",
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=2) # K近傍法を行うためのクラス\n",
    "knn.fit(train_data, train_label) # 学習\n",
    "knn.predict([[1.5]]) # 予測\n",
    "knn.predict_proba([[1.5]]) # 結果に対する確率\n",
    "knn.score(X_test, Y_test) # 予測結果とラベルが同じである確率を取得\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10) # K分割交差検定用のクラス\n",
    "for train, test in cv.split(X): ... # のようにして使う\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs') # ロジスティック回帰を行うクラス．fit, predict等のインタフェースはneighborsと同じ．\n",
    "clf.intercept_ # ロジスティック回帰のバイアスの部分を取得\n",
    "clf.coef_ # ロジスティック回帰の係数を取得\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc, classification_report\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, clf.predict(X_test)) # PR曲線のthresholdごとのプロットデータを取得\n",
    "auc(recall, precision) # PRCurveの線下面積を取得\n",
    "classification_report(y_test, clf.predit_proba[:, 1] > 0.63, target_names=['not accepted', 'accepted']) # 適合率と再現率の値の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 6章 ナイーブベイズによる文書分類とパラメータ最適化\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB(身長体重などの正規分布を仮定), MultinomialNB, BernoulliNB(単語が出現した/しないの2値を仮定)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = MultinomialNB() # TF-IDFと相性が良い，カウントベースのナイーブベイズモデル\n",
    "pipeline = Pipeline([ ('vect', TfidfVectorizer()), ('clf', clf)]) # データを入れると，まずTF-IDFに変換してから学習を行うパイプラインを作成\n",
    "# ↑ここでパイプラインに名前がついているのは，パラメータ探索を自動で行うGridSearchで使えるから．\n",
    "pipeline.steps # パイプラインの内容を取得\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "rs = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0) # KFoldとは違ってランダムにデータを分割する交差検定クラス．使い方は同じ．\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# パイプライン名__引数名=[探索する値]\n",
    "param_grid = dict(\n",
    "    vect__ngram_range=[(1, 1), (1, 2), (1, 3)], \n",
    "    vect__min_df=[1, 2],\n",
    "    ...\n",
    "    clf__alpha=[0, 0.01, 0.05, 0.1, 0.5, 1],\n",
    ")\n",
    "# Pipelineで名付けたvect, clfに対して，param_gridで指定したパラメータを使ってグリッドサーチする．\n",
    "scorer = make_scorer(f1_score) # f値によるスコア計算器\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv = rs, scoring=scorer, verbose=10) # verboseの値に応じて経過出力\n",
    "grid_search.fit(X, Y) # パラメータ探索開始\n",
    "grid_search.best_estimator_ # 最適パラメータを取得\n",
    "\n",
    "import nltk\n",
    "token = nltk.word_tokenize(\"This is a book.\") # 単語分割, トークン化(要punkt)\n",
    "nltk.pos_tag(token) # トークンの文章構造からタグをつける．(要averaged_perceptron_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 7章 ペナルティ項を持つ線形回帰モデルによる数値予測\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston() # ボストンの物件価格を予想するデータセット\n",
    "boston.DESCR # データの説明\n",
    "boston.feature_names # 特徴量の名前一覧\n",
    "\n",
    "(slope, bias), error, rank, singular_value = np.linalg.lstsq(x, y, rcond=None) # 最小二乗法, rcondはワーニング対策\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge, ElasticNetCV, LassoCV, RidgeCV\n",
    "lr = LinearRegression(fit_intercept=True) # 線形回帰モデル．fit, predictを持つ．fit_intercept: バイアス項を追加\n",
    "en = ElasticNet(fit_intercept=True, alpha=0.5)  # ペナルティ項を持つElastic Netによる線形回帰モデル．alphaはペナルティ項の係数．あとl1_ratio\n",
    "met = ElasticNetCV(alphas=[.125, .25, .5, 1., 2., 4.]) # CVによる最適パラメータ探索つきElasticNet\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "data, target = load_svmlight_file('./data/E2006.train') # SVM関連のツール用フォーマットのロード\n",
    "\n",
    "from scipy import sparse # 2次元スパース行列のためのパッケージ\n",
    "reviews = sparse.csc_matrix((values, ij.T)).astype(float) # Compressed Sparse Column matrix\n",
    "reviews.todense() # 疎行列形式から通常の行列に戻す\n",
    "reviews.toarray() # 疎行列形式から通常の配列に戻す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 8章 回帰の改良，バスケット分析など，手実装が多かったのであまり新しいライブラリの関数は出てこない\n",
    "np.corrcoef(a, b) # aとbの相関係数．a, bが2次元なら相関係数のリストを返してくれる．\n",
    "\n",
    "from itertools import chain\n",
    "chain(*dataset) # 複数のイテレータを連続して一つにまとめたイテレータにする．\n",
    "\n",
    "import pymining # 先進的なデータマイニングアルゴリズムのパッケージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 9章 FFT，MFCCを使った音楽分類と混合行列によるテスト結果の可視化\n",
    "\n",
    "from scipy.io import wavfile\n",
    "sample_rate, X = wavfile.read(wave_filename) # WAV形式ファイルのサンプリングレートと波形データを取得\n",
    "plt.specgram(X, Fs=sample_rate, xextent=(0, len(X) // sample_rate)) # matplotlibでスペクトル表示\n",
    "\n",
    "import scipy as sp\n",
    "sp.fft(wave) # 高速フーリエ変換．np.fft.fftでもできる．\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pylab\n",
    "cm = confusion_matrix(y_test, y_pred) # どのデータがどこに分類されたのかを可視化する混合行列を作成\n",
    "pylab.matshow(cm, fignum=False, cmap=\"Blues\") # 混合行列を表示\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(test, pred) # PR曲線のthresholdごとのプロットデータを取得\n",
    "\n",
    "from librosa.feature import mfcc\n",
    "mfccs = mfcc(np.array(X, dtype=float), sr=sr) # メル周波数ケプストラム係数特徴量を取得．floatのnparrayである必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 10章 画像処理と画像の特徴量\n",
    "\n",
    "import mahotas as mh\n",
    "image = mh.imread('./sample.jpg') # 画像データをnumpy配列として取得\n",
    "plt.imshow(image) # 画像データの出力．as_grey引数でグレースケールにするかを指定\n",
    "thresh = mh.thresholding.otsu(gray_image) # 大津の二値化による閾値の取得\n",
    "thresh = mh.thresholding.rc(gray_image) # Ribler-Calvard手法による二値化閾値の取得\n",
    "im8 = mh.gaussian_filter(gray_image, 8) # ガウシアンフィルタをかける．8はフィルタのサイズ(フィルタの標準偏差)で，大きいほどぼやける\n",
    "image = mh.stretch(image) # 最大255, 最小0の画像に変換する\n",
    "\n",
    "mh.features.haralick(image) # Haralickテクスチャ特徴量を取得．上下左右の4方向の特徴量が返ってくるが，これらを平均してもよい．\n",
    "sobel_filtered = mh.sobel(image, just_filter=True) # Sobelフィルタによるエッジ検出\n",
    "\n",
    "from mahotas.features import surf\n",
    "surf_descriptors = surf.surf(grey_sample, descriptor_only=True) # SURFによる記述子の取得．onlyで特徴量の場所，サイズ，などを受け取らない\n",
    "dense_descriptors = surf.dense(grey_sample, spacing=16) # 16ピクセル間隔のグリッド上の点における記述子, 特徴量の場所などは固定なので出力されない\n",
    "# この記述子をkmeansでクラスタリングし，各クラスタにいくつ属するかを特徴量とする\n",
    "\n",
    "Y, X = np.mgrid[:h, :w] # (h, w)の2次元配列を2つ生成，Y: 行に同じ数, X: 列に同じ数， となっており，(X, Y)が組み合わせになっている．    \n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "cross_val_score(LogisticRegression(), features, labels, cv=5).mean() # 交差検定でのスコアリストの平均を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 11章 次元削減，RFE, PCA，LDA，MDS\n",
    "\n",
    "from scipy.stats import pearsonr, entropy\n",
    "pearsonr([1,2,3], [1,2,3.1]) # 相関係数とP値を取得．P値はその相関係数が偶然発生したものである可能性，すなわち信頼できなさ．\n",
    "entropy([0.5, 0.5], base=2) # 確率分布の自己情報量，エントロピーを取得．baseのデフォルトは自然対数\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=3, random_state=0) # 人口データセットの作成\n",
    "selector = RFE(clf, n_features_to_select=n_features_to_select) # ラッパー法であるRFEによる次元削減を行うクラス\n",
    "selector = selector.fit(X, y) # 与えられたモデルを訓練\n",
    "selector.support_ # n_features_to_selectで指定された数だけ特徴量を選ばれている．選ばれているかどうかのリストを取得\n",
    "selector.ranking_ # 特徴量役に立つランキングを取得\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "pca = decomposition.PCA(n_components=1) # 主成分分析(PCA)を行うためのクラス．次元削減後の次元を指定．\n",
    "Xtrans = pca.fit_transform(X) # 変換後の特徴空間に射影, fitメソッドとtransformメソッドの組み合わせ．\n",
    "pca.explained_variance_ratio_ # 変換後，どれだけの分散が維持されているか(次元が減ると一般的に分散は減るみたい)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda_inst = LDA(n_components=1) # 線形判別分析(LDA)を行うためのクラス．クラス外の分散を最大化，クラス内の分散を最小化する．\n",
    "Xtrans = lda_inst.fit_transform(X, labels) # フィッティングにクラスラベルを利用する，教師ありの手法\n",
    "\n",
    "from sklearn import manifold\n",
    "mds = manifold.MDS(n_components=3) # 多次元尺度構成法(MDS)を行うためのクラス．次元削減後の相対距離を保つようにする．\n",
    "Xtrans3 = mds.fit_transform(X) # 2次元や3次元に変換して，可視化に使える．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 12章 jugとAWS\n",
    "\n",
    "from jug import Task, TaskGenerator\n",
    "t1 = Task(lambda x: x * 2, 3) # 計算データのキャッシュを取るタスクとして関数を実行\n",
    "\n",
    "@TaskGenerator # 次の関数はいつでもタスクとして実行\n",
    "def double(x):\n",
    "    sleep(4)\n",
    "    return 2 * x\n",
    "\n",
    "# 以上のようにして学習を行うスクリプトをjugfile.pyで作成し，jug executeを実行するとスクリプトが実行され，キャッシュを取りながら処理を行う\n",
    "\n",
    "# starcluster コンフィグファイルの作成，sshキーを作成，AWSのサーバをひとまとめにしたクラスタの作成，削除，マスターノードにログインなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用したデータセットのまとめ\n",
    "\n",
    "- 1章\n",
    "    - web_traffic.tsv: ウェブトラフィックのデータ\n",
    "- 2章\n",
    "    - アイリスデータセット: アヤメの形状から品種を推定するデータ．sklean.datasets.load_iris\n",
    "- 3章\n",
    "    - 20newsgroup: 様々な分野のニュース記事: sklearnのデータセット取得機能で取得\n",
    "- 4章\n",
    "    - Associated Pressの標準的なニュースデータセット: wget http://www.cs.columbia.edu/~blei/lda-c/ap.tgz\n",
    "    - Wikipediaのダンプデータ: wget http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 --no-check-certificate\n",
    "- 5章\n",
    "    - stackoverflowのジャンルごとの質問と回答集: https://archive.org/details/stackexchange\n",
    "- 6章\n",
    "    - ツイート with 感情データ: https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv\n",
    "- 7章\n",
    "    - ボストンの物件の価格を予想するデータ．sklearn.datasets.load_boston\n",
    "    - 10-K reports: アメリカの公式決算報告書．株価予測: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/E2006.train.bz2\n",
    "    - GroupLens: ユーザと映画のスコアデータ．: wget http://www.grouplens.org/system/files/ml-100k.zip --no-check-certificate\n",
    "- 8章\n",
    "    - ベルギーのスーパーマーケットの買い物データ．商品は匿名．: wget http://fimi.ua.ac.be/data/retail.dat.gz\n",
    "- 9章\n",
    "    - GTZAN: 短い100曲×10ジャンルの音楽データ．: wget http://opihi.cs.uvic.ca/sound/genres.tar.gz\n",
    "- 10章\n",
    "    - SimpleImageDataset: サポートページに置かれた画像データセット: https://github.com/luispedro/BuildingMachineLearningSystemsWithPython\n",
    "    - 動物，車，交通機関，風景の画像データセット: wget http://vision.stanford.edu/Datasets/AnimTransDistr.rar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
