{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "関連する文書を素早く見つけるため，文書のクラスタリング(教師なし学習)を行った．\n",
    "\n",
    "- 文書の前処理\n",
    "    - bag-of-words化: 単語の出現回数を数え，その文書の特徴とする，Scikit-Learnでできる\n",
    "        - 同じ文章が繰り返されているだけで単語の重要度が上がってしまうので，出現回数の正規化を行う\n",
    "        - min_df, max_df: 滅多に出現しない単語，非常に多く一般的に使われる単語を取り除く\n",
    "        - stop words: その言語体系で文書の分類に貢献しないような一般的な単語を取り除く\n",
    "        - ステミング: 語幹を数える (ex. imaging -> image)\n",
    "        - TF-IDF: その単語がどれだけすべての文書の中で特徴的かを表す指標，Scikit-Learnで算出できる\n",
    "    - N-gram: not eatのように否定を表したりすることを認識させたいならば，前後するペア(バイグラム)や3つの単語(トリグラム)を利用する\n",
    "- クラスタリング手法\n",
    "    - 新しい文書を分類するとき，bag-of-wordsのノルムを見れば良いだけになり，素早い分類が可能になる\n",
    "    - フラットクラスタリング: 全てのデータが高々１つのクラスタに所属\n",
    "        - KMeans: 広く使われるクラスタリング手法\n",
    "            1. クラスタの数num_clustersを指定して初期化\n",
    "            1. num_clustersの数だけランダムにデータを選び出し，それらの特徴ベクトルを各クラスタの中心点とする\n",
    "            1. その他のデータは，もっとも近い中心点を持つクラスタに所属させる\n",
    "            1. 全てのデータのクラスタが決まったら，各クラスタの中心を新たに計算し，そこに中心点を移動させる\n",
    "            1. 中心点の移動量が閾値を下回る(収束)，または指定の繰り返し回数に達するまで2~4を繰り返す\n",
    "    - 階層的クラスタリング: クラスタのクラスタ，のように階層構造を持つ\n",
    "    - コサイン類似度，ピアソンの相関係数，Jaccard係数などの他の類似度の指標を使ったりしてマイナーチェンジしてみてもいい\n",
    "    - 良いクラスタリングの指標を明確にするのが大事．代表的な指標はsklearn.metrixにある．\n",
    "\n",
    "文書のクラスタリングをするときの流れ\n",
    "1. 文書データを集める\n",
    "1. 文書データをScikit-LearnでVectorizeする\n",
    "1. Vectorizeされた文書をScikit-LearnのKMeansなどでクラスタリング\n",
    "1. 新しい文書データを分類する\n",
    "    1. 新しい文書データをVectorizeする\n",
    "    1. クラスタリング結果を持つKMeansに文書をPredictさせると，新しい文書データの所属するクラスタが素早くわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前の章ではデータに正解ラベルがついていた．これを**教師あり学習** という．  \n",
    "ここでは，ラベルがない場合，**教師なし学習**を考えてみる．  \n",
    "<br>\n",
    "教師なし学習では，データだけからあるパターンを見つける．  \n",
    "例えば，質問サイトで，ある質問に関連するページを見つけるには，文書ごとの類似度を全て算出すれば良いと考えられる．  \n",
    "しかしこの方法は計算量が膨大になってしまう．  \n",
    "素早く関連する文書を見つけるためには，**クラスタリング**という手法が使えそうである．  \n",
    "こちらもテキストデータ同士で類似度を算出する方法が必要になるが，ここではSciKitライブラリの機能を使ってみたいと思う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書の関連性を計測する\n",
    "まずテキストデータを意味のある数字に変換したい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やってはいけないこと\n",
    "テキストデータの類似度を求めるには，**レーベンシュタイン距離(編集距離とも呼ばれる)** が利用できそう．  \n",
    "文字ごとの編集距離を求めると時間がかかり過ぎてしまうので，単語を最小単位として扱い，文章全体で編集距離を計測しようとも思ってみる．  \n",
    "しかし依然編集距離のオーダーは大きく，時間がかかる．  \n",
    "また，同じ単語が2つの文書に出現していて，位置が異なる場合にも編集距離は発生してしまうので，この方法はロバストであるとは言えない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どうやるべきか\n",
    "**bag-of-words**を使ってみる．  \n",
    "単語の出現回数を特徴量として扱うことで，1つの文書を1つのベクトルとして扱える．  \n",
    "(しかし，この方法で最近傍点を見つけるのには時間がかかりすぎる．)  \n",
    "**bag-of-words**によってクラスタリング処理を行う手順を示す．  \n",
    "1. 各文書から特徴量を抽出し，特徴ベクトルの形で保存する\n",
    "1. 特徴ベクトルに対してクラスタリングを行う\n",
    "1. 投稿された質問文書に対して，クラスタを決定する\n",
    "1. このクラスタに属する文書を他に集め，多様性を増す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理: 共通する単語の出現回数を類似度として計測する\n",
    "scikitのCountVectorizerでbag-of-wordsを作れる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_dfはその数より出現回数の小さい単語を無視する．  \n",
    "analizerは，単語レベルで出現回数のカウントを行なっていることを意味する．  \n",
    "token_pattenでは単語の決定方法を定義．例えばcross-validatedをcrossとvalidatedに分けるかを設定できる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]] \n",
      "\n",
      " [[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "print(\"vocabulary:\", vectorizer.get_feature_names())\n",
    "print(X.toarray(), \"\\n\\n\", X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語を数える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"This is a toy post about machine learning. Actually, it contains not much interesting stuff.\"\n",
    "txt2 = \"Imaging databases provide storage capabilities.\"\n",
    "txt3 = \"Most imaging databases safe images permanently.\"\n",
    "txt4 = \"Imaging databases store data.\"\n",
    "txt5 = \"Imaging databases store data. Imaging databases store data. Imaging databases store data.\"\n",
    "\n",
    "posts = [txt1, txt2, txt3, txt4, txt5]\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"#samples: {num_samples}, #features: {num_features}\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# 新しい文書のベクトル化\n",
    "new_post = [\"imaging databases\"] # Iterable over raw text documents expected, string object received.対策\n",
    "new_post_vec = vectorizer.transform(new_post)\n",
    "print(new_post_vec) # 疎なベクトルなので，0でない値のみが表示される\n",
    "print(new_post_vec.toarray()) # ベクトル全体を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つの文書のユークリッド距離を計算してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist 1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def distances(posts, new_post, distance):\n",
    "    best_doc = None\n",
    "    best_dist = sys.maxsize\n",
    "    best_i = None\n",
    "    new_post_vec = vectorizer.transform(new_post)\n",
    "    for i in range(num_samples):\n",
    "        post = posts[i]\n",
    "        if post == new_post: continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        d = distance(post_vec, new_post_vec)\n",
    "        print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"Best post is %i with dist %.2f\" % (best_i, best_dist))\n",
    "\n",
    "distances(posts, new_post, dist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語の出現回数ベクトルを正規化する\n",
    "Post3とPost4の結果は同じにならないと不自然なので，正規化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist 0.77\n"
     ]
    }
   ],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要度の低い単語を取り除く\n",
    "文書の分類に貢献しない単語，**ストップワード** を取り除く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print(sorted(vectorizer.get_stop_words())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist 0.77\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステミング\n",
    "語形変化に対応し，語幹(stem)を単語として扱いたい．  \n",
    "scikitにはその機能がないため，**Natural Language Toolkit(NLTK)** が必要\n",
    "```\n",
    "$ pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphics -> graphic\n",
      "imaging -> imag\n",
      "image -> imag\n",
      "imagination -> imagin\n",
      "imagine -> imagin\n",
      "buy -> buy\n",
      "buying -> buy\n",
      "bought -> bought\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "stemmer = nltk.stem.SnowballStemmer('english') # 他にも色々ステマーはあるらしい\n",
    "\n",
    "words = [\"graphics\", \"imaging\", \"image\", \"imagination\", \"imagine\", \"buy\", \"buying\", \"bought\"]\n",
    "stemmed = [stemmer.stem(w) for w in words]\n",
    "for w, s in zip(words, stemmed): print(w, \"->\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ステミングを行い，トークン化と正規化を行うようCountVectorizerのbuild_analizerメソッドをオーバーライド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist 0.63\n"
     ]
    }
   ],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer() # 文書を小文字に変換 + 単語の抜き出し\n",
    "        return lambda doc: (stemmer.stem(w) for w in analyzer(doc)) # ステミング\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFを用いる\n",
    "頻出単語はmax_dfを0.9に設定するなどすることで除外できるが，0.89は除外できない．  \n",
    "この閾値を考えているとキリがない．  \n",
    "また，これでは除外されなかった頻出単語とそうでない単語で識別性能(クラスタリングへの貢献度)が変わってきてしまう．  \n",
    "そこで，文書内での単語の重要度として，単語の出現数の代わりに**TF-IDF(term frequency - inverse document frequency)** を使ってベクトル化する．  \n",
    "<br>\n",
    "tf: Term Frequency, 与えられた文書の中での単語の出現頻度\n",
    "$$ tf = \\frac{文書docにおける単語termの出現頻度}{文書docにおける全単語の出現頻度の和(docの単語数?)} = 文書docにおける単語termの割合$$\n",
    "<br>\n",
    "idf: Inverse Document Frequency, 逆文書頻度,単語が「レア」なら高い値を，「他の文書にも良く出現する単語」なら低い値を示すもの．  \n",
    "レアな単語はその文書の特徴を判別するのに有用であるとする．  \n",
    "$$ idf = \\log \\Bigl( \\frac{全文書数}{単語termを含む文書数} \\Bigr) = - \\log \\Bigl( \\frac{単語termを含む文書数}{全文書数} \\Bigr) $$\n",
    "これは単語termを含むという事象に対する情報量，エントロピーの式になっている．  \n",
    "この文書が単語termを含む珍しさ，含むことの予測しにくさを表している．  \n",
    "そして，TF-IDFは\n",
    "$$ tfidf = tf \\times idf $$\n",
    "で表される．  \n",
    "<br>\n",
    "TF-IDFによって，「その単語がその文書でよく出現するほど」，「その単語が他の文書と比べてレアなほど」大きな値になる．  \n",
    "つまり，TF-IDFはその単語が文書全体の中でどれだけ特徴的かを表す指標である．  \n",
    "その文書内にその単語が出現しなければ,TF-IDFは0になる．  \n",
    "参考: https://dev.classmethod.jp/machine-learning/yoshim_2017ad_tfidf_1-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term)) / sum(doc.count(w) for w in set(doc))\n",
    "    idf = math.log(float(len(docset)) / (len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.27031007207210955\n",
      "0.0\n",
      "0.13515503603605478\n",
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "doc1, doc2, doc3 = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [doc1, doc2, doc3]\n",
    "print(tfidf(\"a\", doc1, D))\n",
    "print(tfidf(\"b\", doc2, D))\n",
    "print(tfidf(\"a\", doc3, D))\n",
    "print(tfidf(\"b\", doc3, D)) # doc2におけるbはdoc3におけるbの2倍重要(2回出てきてるから)\n",
    "print(tfidf(\"c\", doc3, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearnにはTF-IDFの値で単語をベクトル化してくれるTfidfVectorizer(CountVectorizerを継承したクラス)が用意されている．  \n",
    "これをCountVectorizerの代わりに使う．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer() # 文書を小文字に変換 + 単語の抜き出し\n",
    "        return lambda doc: (stemmer.stem(w) for w in analyzer(doc)) # ステミング\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここまでやってきたこと\n",
    "1. 文書データをトークン化(bag-of-words化)する\n",
    "1. 頻出しすぎる単語は取り除く (stop wordsの除外)\n",
    "1. 滅多に使われない単語は，新しい文書でも使われる可能性が低いため，取り除く(min_dfの設定)\n",
    "1. 残った単語について，その出現回数をカウントする\n",
    "1. ↑のカウント手法だと単語ごとにクラスタリングへの貢献度が変わってきてしまうので，TF-IDFを文書の特徴ベクトルの要素とする．\n",
    "\n",
    "このbag-of-wordsによるアプローチは，単純でありながら，優れた性能を持つ．  \n",
    "しかし，以下のような欠点を持つ．  \n",
    "- 単語の前後関係について考慮していない．たとえば，Car hits wall と Wall hits carが同じものとして扱われる．\n",
    "- 否定的な意味を捉えられない． たとえば．I will eat ice creamと I will not eat ice cream の特徴ベクトルが非常に似ることになる．  \n",
    "    - この問題は，単語を個別に考えるのではなく，ペア(バイグラム)や3つの連続する単語(トリグラム)を一つのまとまりとしてカウントすることで解決できる．  \n",
    "- タイプミスに対応できない． databaseとdatabasが別の単語として扱われる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング\n",
    "ここでようやく文書の内容を適切に表現しているであろう特徴ベクトルを手にすることができた．  \n",
    "クラスタリング手法の多くは，次の2つの手法のどちらかに該当する．  \n",
    "- フラットクラスタリング(flat clustering)\n",
    "    - クラスタ間の関係を考慮しない\n",
    "    - 全てのデータがどこか1つのクラスタに属するよう分割\n",
    "    - 前もってクラスタの数を決める必要があるアルゴリズムが多い\n",
    "- 階層的クラスタリング(hierarchical clustering)\n",
    "     - 普通にフラットクラスタリングを行なった後，さらに近いクラスタ同士を一つのクラスタにまとめる親クラスタを作ることを再帰的に繰り返す\n",
    "     - 最終的には全てのデータが一つの大きなクラスタに属すことになる\n",
    "     - 前もってクラスタの数を決める必要はない． 決めることはできるが，処理効率を落とすことになるらしい\n",
    " \n",
    "Scikit-learnでできる様々なクラスタリング手法の説明  \n",
    "https://scikit-learn.org/dev/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "もっとも広く用いられるフラットクラスタリング手法\n",
    "1. クラスタの数num_clustersを指定して初期化\n",
    "1. num_clustersの数だけランダムにデータを選び出し，それらの特徴ベクトルを各クラスタの中心点とする\n",
    "1. その他のデータは，もっとも近い中心点を持つクラスタに所属させる\n",
    "1. 全てのデータのクラスタが決まったら，各クラスタの中心を新たに計算し，そこに中心点を移動させる\n",
    "1. 中心点の移動量が閾値を下回る(収束)，または指定の繰り返し回数に達するまで2~4を繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch03/plot_kmeans_example.pyを参照，実行  \n",
    "コアな部分は以下に示す．Scikit-learnを使っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) \n",
      " [0.24794698 0.12655967 0.26822595 0.34250824 0.15058082]\n",
      "(20,) \n",
      " [0.36760646 0.08304103 0.37053545 0.38239828 0.06558156]\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 0.3206469287517518\n",
      "[2 1 2 0 1 1 1 2 1 0 2 0 0 0 2 0 0 0 2 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3\n",
    "seed = 2\n",
    "\n",
    "x = norm(loc=0.3, scale=.15).rvs(20) # NORMal distributionからRandom VariableS 20個を取得\n",
    "y = norm(loc=0.3, scale=.15).rvs(20)\n",
    "\n",
    "print(x.shape, \"\\n\", x[:5])\n",
    "print(y.shape, \"\\n\", y[:5])\n",
    "\n",
    "# KMeansインスタンスを生成\n",
    "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
    "            n_init=1, max_iter=1,\n",
    "            random_state=seed)\n",
    "\n",
    "km.fit(sp.array(list(zip(x, y)))) # クラスタリングを開始\n",
    "\n",
    "# クラスタリングの結果を表示\n",
    "labels = km.predict(list(zip(x, y)))\n",
    "print(labels)\n",
    "\n",
    "x = [x[labels == i] for i in range(num_clusters)]\n",
    "y = [y[labels == i] for i in range(num_clusters)]\n",
    "\n",
    "for x_i, y_i, color in zip(x, y, [\"red\", \"blue\", \"green\"]):\n",
    "    plt.scatter(x_i, y_i, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![01](ch03/1400_03_01.png)\n",
    "![02](ch03/1400_03_02.png)\n",
    "![03](ch03/1400_03_03.png)\n",
    "![04](ch03/1400_03_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータを用いて評価を行う\n",
    "20newsgroupというデータセットを利用する.  \n",
    "scikit-leanのデータセット取得機能を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Programming\\\\machinelearning\\\\BuildingMachineLearningSystems\\\\data\\\\20news_home\\\\20news-bydate-train\\\\sci.space\\\\61195'\n",
      " 'D:\\\\Programming\\\\machinelearning\\\\BuildingMachineLearningSystems\\\\data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\51837'\n",
      " 'D:\\\\Programming\\\\machinelearning\\\\BuildingMachineLearningSystems\\\\data\\\\20news_home\\\\20news-bydate-train\\\\comp.os.ms-windows.misc\\\\9718'\n",
      " ...\n",
      " 'D:\\\\Programming\\\\machinelearning\\\\BuildingMachineLearningSystems\\\\data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.ibm.pc.hardware\\\\60300'\n",
      " 'D:\\\\Programming\\\\machinelearning\\\\BuildingMachineLearningSystems\\\\data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\50471'\n",
      " 'D:\\\\Programming\\\\machinelearning\\\\BuildingMachineLearningSystems\\\\data\\\\20news_home\\\\20news-bydate-train\\\\comp.windows.x\\\\66965']\n",
      "3529\n",
      "['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
      "2349\n",
      "3529\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    data_home = \"D:\\Programming\\machinelearning\\BuildingMachineLearningSystems\\data\" # for windows\n",
    "else: \n",
    "    data_home = \"/Users/komatsu/programing/machinelearning/BuildingMachineLearningSystems/data\" # for Mac\n",
    "    \n",
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
    "data = fetch_20newsgroups(data_home=data_home, categories=categories) # 最初は数分かかる\n",
    "\n",
    "print(data.filenames)\n",
    "print(len(data.filenames))\n",
    "print(data.target_names)\n",
    "\n",
    "test_data = fetch_20newsgroups(data_home=data_home, categories=categories, subset=\"test\")\n",
    "print(len(test_data.filenames))\n",
    "\n",
    "train_data = fetch_20newsgroups(data_home=data_home, categories=categories, subset=\"train\")\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書のクラスタリング\n",
    "まずは不適切な文字を無視する．  \n",
    "(不適切な文字が含まれるとき，UnicodeDecodeErrorなどが発生)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529, $features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, stop_words='english', decode_error='ignore') # UnicodeDecodeErrorが発生したら無視\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(f\"#samples: {num_samples}, #features: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "早速クラスタリングを実施していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5943.138\n",
      "Iteration  1, inertia 3206.928\n",
      "Iteration  2, inertia 3162.585\n",
      "Iteration  3, inertia 3142.215\n",
      "Iteration  4, inertia 3130.749\n",
      "Iteration  5, inertia 3122.989\n",
      "Iteration  6, inertia 3118.256\n",
      "Iteration  7, inertia 3115.722\n",
      "Iteration  8, inertia 3113.928\n",
      "Iteration  9, inertia 3112.969\n",
      "Iteration 10, inertia 3112.164\n",
      "Iteration 11, inertia 3111.479\n",
      "Iteration 12, inertia 3111.033\n",
      "Iteration 13, inertia 3110.408\n",
      "Iteration 14, inertia 3110.099\n",
      "Iteration 15, inertia 3110.029\n",
      "Iteration 16, inertia 3109.933\n",
      "Iteration 17, inertia 3109.686\n",
      "Iteration 18, inertia 3109.588\n",
      "Iteration 19, inertia 3109.468\n",
      "Iteration 20, inertia 3109.392\n",
      "Iteration 21, inertia 3109.332\n",
      "Iteration 22, inertia 3109.289\n",
      "Converged at iteration 22: center shift 0.000000e+00 within tolerance 2.069005e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=50, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init=\"random\", n_init=1, verbose=1)\n",
    "km.fit(vectorized) # vectorizedは3529個の4712次元の点(2次元配列)なので，そのままfitにぶちこめる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各文書がどのクラスタに属したのかは以下のようにして表示できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 48 46 ...  4 41 28]\n",
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)\n",
    "print(km.labels_.shape) # 文書の数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最初の問題に対する答え\n",
    "新しい文書をうまく分類できるか確かめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disk drive problems. Hi, I have a problem with my hard disk.\n",
      "After 1 year it is working only sporadically now.\n",
      "I tried to format it, but now it doesn't boot any more.\n",
      "Any ideas? Thanks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_post = \"\"\"\n",
    "Disk drive problems. Hi, I have a problem with my hard disk.\n",
    "After 1 year it is working only sporadically now.\n",
    "I tried to format it, but now it doesn't boot any more.\n",
    "Any ideas? Thanks.\n",
    "\"\"\"\n",
    "print(new_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0] # すでにクラスタリングは行っているので，他の文書のすべてと比べる必要はない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "[  71  218  292  379  630  661  666  768  854  882  884 1114 1260 1377\n",
      " 1486 1600 1602 1809 1837 1942 1985 2024 2085 2231 2424 2458 2510 2664\n",
      " 2682 2687 2889 2944 3080 3111 3119 3145 3154 3333 3369 3437]\n"
     ]
    }
   ],
   "source": [
    "similar_indicates = (km.labels_ == new_post_label).nonzero()[0]\n",
    "print(len(similar_indicates))\n",
    "print(similar_indicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 3], dtype=int64),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([True, False, True, True]).nonzero() # nonzeroで条件を満たすインデックスを取得できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_postと同じクラスタに属す文書はどれだけnew_postに似ているのだろうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0494693076510362 \n",
      " From: rogntorb@idt.unit.no (Torbj|rn Rognes)\n",
      "Subject: Adding int. hard disk drive to IIcx\n",
      "Keywords: Mac IIcx, internal, hard disk drive, SCSI\n",
      "Reply-To: rogntorb@idt.unit.no (Torbj|rn Rognes)\n",
      "Organization: Div. of CS & Telematics, Norwegian Institute of Technology\n",
      "Lines: 32\n",
      "\n",
      "I haven't seen much info about how to add an extra internal disk to a\n",
      "mac. We would like to try it, and I wonder if someone had some good\n",
      "advice.\n",
      "\n",
      "We have a Mac IIcx with the original internal Quantum 40MB hard disk,\n",
      "and an unusable floppy drive. We also have a new spare Connor 40MB\n",
      "disk which we would like to use. The idea is to replace the broken\n",
      "floppy drive with the new hard disk, but there seems to be some\n",
      "problems:\n",
      "\n",
      "The internal SCSI cable and power cable inside the cx has only\n",
      "connectors for one single hard disk drive.\n",
      "\n",
      "If I made a ribbon cable and a power cable with three connectors each\n",
      "(1 for motherboard, 1 for each of the 2 disks), would it work?\n",
      "\n",
      "Is the IIcx able to supply the extra power to the extra disk?\n",
      "\n",
      "What about terminators? I suppose that i should remove the resistor\n",
      "packs from the disk that is closest to the motherboard, but leave them\n",
      "installed in the other disk.\n",
      "\n",
      "The SCSI ID jumpers should also be changed so that the new disk gets\n",
      "ID #1. The old one should have ID #0.\n",
      "\n",
      "It is no problem for us to remove the floppy drive, as we have an\n",
      "external floppy that we can use if it won't boot of the hard disk.\n",
      "\n",
      "Thank you!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Torbj|rn Rognes                            Email: rogntorb@idt.unit.no\n",
      " \n",
      "\n",
      "1.285143752149181 \n",
      " From: u96_bbayraml@vaxc.stevens-tech.edu\n",
      "Subject: FOR SALE!! DECpc325sxLP\n",
      "Lines: 26\n",
      "Organization: Stevens Institute Of Technology\n",
      "\n",
      "\n",
      "\n",
      "      FOR SALE !!!\n",
      "\n",
      "      DECpc 325sxLP\n",
      "\n",
      "   It's in very good condition, used for one year. It has\n",
      "\n",
      "      - 25 Mhz Intel 386\n",
      "      - 52 MB Hard Disk\n",
      "      - Super Color VGA Monitor\n",
      "      - 2-button mouse\n",
      "      - 1.44 MG floppy disk drive\n",
      "\n",
      "      Software:\n",
      "    ------------\n",
      "\n",
      "       - Microsoft Dos 5.0\n",
      "       - Microsoft Windows 3.1\n",
      "       - Microsoft Works for Windows 2.0\n",
      "       - Borland Turbo Pascal 6.0\n",
      "       - Borland Turbo C++ 3.0 for Dos\n",
      "       \n",
      "\n",
      "       I'm asking $1499 for the system. Send me E-mail if interested.\n",
      "      \n",
      " \n",
      "\n",
      "1.3424188133468438 \n",
      " From: lgorbet@triton.unm.edu (Larry P Gorbet ANTHROPOLOGY)\n",
      "Subject: Re: Floptical Question\n",
      "Organization: University of New Mexico, Albuquerque\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: triton.unm.edu\n",
      "\n",
      "In article <bmyers-140493201843@slip-x27.ots.utexas.edu> bmyers@ccwf.cc.utexas.edu (Billy Lee Myers, Jr.) writes:\n",
      ">...the last time I looked, floptical disk\n",
      ">weren't all that cheap, ($30 per floptical disk = $1.40 per megabyte, $60\n",
      ">per sysquest is $1.36 per megabyte).\n",
      "\n",
      "Flopticals have been available since the beginning of the year at $25\n",
      "per floptical (= $1.20 per megabyte), and I have seen them advertised\n",
      "in MacWEEK at $20 (< $1.00 per megabyte).  For someone on a tight\n",
      "budget, the fact that the minimal dollar increment for more storage is\n",
      "less---$25 versus $60---sometimes matters.\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar = sorted([(sp.linalg.norm((new_post_vec - vectorized[i]).toarray()), train_data.data[i]) for i in similar_indicates])\n",
    "print(similar[0][0], \"\\n\", similar[0][1], \"\\n\") # 最初\n",
    "print(similar[len(similar)//2][0], \"\\n\", similar[len(similar)//2][1], \"\\n\") # まんなか\n",
    "print(similar[-1][0], \"\\n\", similar[-1][1], \"\\n\") # 最後"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノイズに対する別の見方\n",
    "クラスタリングの結果はニュースグループの分け方になるわけではない．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(160, 'From: passman@world.std.com (Shirley L Passman)\\nSubject: help with no docs for motherboard\\nOrganization: The World Public Access UNIX, Brookline, MA\\nLines: 1\\n\\n\\n', 'comp.sys.ibm.pc.hardware'), (162, 'Subject: E-mail of Michael Abrash?\\nFrom: gmontem@eis.calstate.edu (George A. Montemayor)\\nOrganization: Calif State Univ/Electronic Information Services\\nLines: 0\\n\\n', 'comp.graphics')]\n",
      "['ntaib', 'silver', 'uc', 'indiana', 'edu', 'iskandar', 'taib', 'subject', 'ms', 'window', 'matur', 'os', 'keyword', 'ms', 'window', 'nntp', 'post', 'host', 'silver', 'uc', 'indiana', 'edu', 'repli', 'comp', 'os', 'ms', 'window', 'advocaci', 'organ', 'indiana', 'univers', 'distribut', 'usa', 'line', '66', 'articl', 'cyen', '735139934', 'ponder', 'jess', 'write', 'hi', 'use', 'mac', 'answer', 'posit', 'know', 'ms', 'window', 'matur', 'os', 'silli', 'unix', 'matur', 'os', 'depend', 'ask', 'defin', 'matur', 'matur', 'window', 'day', 'ago', 'peopl', 'doubt', 'ms', 'window', 'real', 'os', 'question', 'ms', 'window', 'confus', 'peopl', 'microsoft', 'simul', 'mac', 'did', 'lousi', 'job', 'exampl', 'creat', 'hierarchi', 'group', 'way', 'creat', 'group', 'group', 'know', 'tell', 'need', 'like', 'behierarch', 'creat', 'group', 'appl', 'menu', 'know', 'appl', 'menu', 'item', 'ripoff', 'program', 'manag', 'want', 'hierarch', 'program', 'launcher', 'lot', 'avail', 'uncomplet', 'document', 'easi', 'reason', 'caus', 'unpredict', 'error', 'easi', 'mac', 'break', 'have', 'spent', 'hour', 'move', 'extens', 'restart', 'mac', 'certain', 'app', 'crash', 'time', 'laughabl', 'group', 'delet', 'file', 'delet', 'delet', 'group', 'user', 'use', 'file', 'manag', 'delet', 'file', 'user', 'forget', 'delet', 'relat', 'file', 'disk', 'nonsens', 'file', 'oh', 'great', 'hear', 'alias', 'wonder', 'appl', 'implement', 'share', 'problem', 'creat', 'window', 'do', 'compil', 'edit', 'languag', 'good', 'editor', 'share', 'problem', 'just', 'open', 'save', 'program', 'load', 'make', 'sens', 'prevent', 'save', 'open', 'eh', 'don', 'follow', 'mean', 'easi', 'satisfi', 'everybodi', 'microsoft', 'want', 'reput', 'evalu', 'user', 'interfac', 'care', 'product', 'distribut', 'mac', 'desktop', 'incred', 'annoy', 'use', 'flame', 'yeah', 'right', 'post', 'flame', 'bait', 'ask', 'flame', 'iskandar', 'taib', 'thing', 'wors', 'peach', 'ala', 'internet', 'ntaib', 'silver', 'uc', 'indiana', 'edu', 'frog', 'frog', 'ala', 'peach', 'bitnet', 'ntaib', 'iubac']\n",
      "['kwilson', 'casbah', 'acn', 'nwu', 'edu', 'kirtley', 'wilson', 'subject', 'mirosoft', 'offic', 'packag', 'articl', 'news', '1993apr6', '183345', '28238', 'organ', 'northwestern', 'univers', 'evanston', 'illinoi', 'line', '31', 'nntp', 'post', 'host', 'unseen1', 'acn', 'nwu', 'edu', 'charg', 'purchas', 'comput', 'softwar', 'small', 'offic', 'question', 'microsoft', 'offic', 'pack', 'wonder', 'program', 'packag', 'offic', 'pack', 'winword', 'power', 'point', 'excel', 'ccmail', 'complet', 'latest', 'addit', 'program', 'hobbl', 'way', 'updat', 'singl', 'program', 'date', 'excel', 'excel', 'receiv', 'necessari', 'disk', 'document', 'awar', 'make', 'offic', 'packag', 'deal', 'sorri', 'skeptic', 'price', 'offic', 'packag', '439', '39', 'street', 'sound', 'like', 'great', 'deal', 'offic', 'purchas', 'comput', 'softwar', 'complet', 'program', '450', 'make', 'just', 'littl', 'suspici', 'mayb', 'just', 'thank', 'help', 'advanc', 'kirt', 'wilson', 'northwestern', 'univers', 'internet', 'kwilson', 'casbah', 'acn', 'nwu', 'edu', 'bitnet', 'kwilson', 'casbah']\n"
     ]
    }
   ],
   "source": [
    "post_group = zip(train_data.data, train_data.target)\n",
    "z = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\n",
    "print(sorted(z)[5:7])\n",
    "\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "print(list(analyzer(z[5][1])))\n",
    "print(list(analyzer(z[6][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果のように，2つの文書の持つ単語から，分類の決め手を見つけることは難しい．  \n",
    "さらに，小文字に変形し，一般的な単語と滅多に登場しない単語を取り除くと次のようになり，さらに分類が難しい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spent', 'don', 'wonder', 'os', 'follow', 'group', 'editor', 'just', 'crash', 'satisfi', 'mean', 'mac', 'save', 'extens', 'file', 'wors', 'forget', 'avail', 'reason', 'ago', 'relat', 'ala', 'host', 'hear', 'lot', 'comp', 'compil', 'articl', 'everybodi', 'hi', 'know', 'restart', 'easi', 'launcher', 'way', 'program', 'hour', 'move', 'product', 'doubt', 'error', 'depend', 'hierarchi', 'app', 'incred', 'write', 'yeah', 'languag', 'univers', 'menu', 'disk', 'exampl', 'implement', 'ms', 'evalu', 'creat', 'like', 'have', 'silver', 'bitnet', 'tell', 'caus', 'do', 'problem', 'alias', 'answer', 'need', 'certain', 'interfac', 'appl', 'microsoft', 'share', 'care', 'good', 'matur', 'document', 'reput', 'item', 'job', 'indiana', 'day', 'prevent', 'did', 'thing', 'internet', 'open', 'right', 'nntp', 'defin', 'ask', 'confus', 'flame', 'make', 'posit', 'edit', 'distribut', 'uc', 'oh', 'silli', 'peopl', 'time', 'eh', 'usa', 'delet', 'sens', 'simul', 'real', 'load', 'manag', 'repli', 'use', 'annoy', 'great', 'break', 'unix', 'want', 'window', '66', 'user', 'keyword', 'question', 'desktop']\n",
      "['packag', 'like', 'illinoi', 'charg', 'wonder', 'addit', 'mayb', 'excel', 'internet', 'deal', 'help', 'nntp', 'point', 'acn', 'purchas', 'receiv', 'make', 'bitnet', 'comput', 'way', 'wilson', 'small', 'necessari', 'singl', 'littl', 'program', 'date', 'power', 'winword', 'just', 'complet', 'offic', 'pack', 'latest', 'thank', 'price', 'great', 'updat', '31', '450', 'microsoft', 'nwu', 'sorri', 'suspici', 'softwar', 'document', 'awar', 'univers', 'street', 'disk', 'host', '39', '1993apr6', 'sound', 'advanc', 'news', 'articl', 'question']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(analyzer(z[5][1])).intersection(vectorizer.get_feature_names())))\n",
    "print(list(set(analyzer(z[6][1])).intersection(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max idf: 6.771157877128972\n",
      "IDF(cs) = 3.1927022406294085\n",
      "IDF(faq) = 4.324866063468751\n",
      "IDF(thank) = 2.187118472770954\n",
      "IDF(bh) = 6.6841465001393425\n",
      "IDF(thank) = 2.187118472770954\n"
     ]
    }
   ],
   "source": [
    "max_idf_id = vectorizer._tfidf.idf_.argmax()\n",
    "max_idf = vectorizer._tfidf.idf_[max_idf_id]\n",
    "print(f\"max idf: {max_idf}\")\n",
    "\n",
    "for term in ['cs', 'faq', 'thank', 'bh', 'thank']:\n",
    "    print(f'IDF({term}) = {vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bhが比較的maxに近いIDFを持っている．  \n",
    "このことから，bh以外はそれほど識別性がないということになる．  \n",
    "そのため，完璧なクラスタリングは難しい．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，ここで我々は新しい文書を分類するとき，bag-of-wordsのノルムを見れば良いだけになり，素早い分類が可能になった．  \n",
    "そして結局，訓練データに用いた文書がどのニュースグループに属するかということに関心があるわけではない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメータの調整\n",
    "他にも手法をマイナーチェンジできるところは色々ある．\n",
    "\n",
    "- クラスタの数\n",
    "- ベクトル化の時のvocabulary数の変更\n",
    "- KMeans以外のクラスタリング手法\n",
    "- コサイン類似度，ピアソン相関係数，Jaccard係数といった，その他の類似度の指標\n",
    "\n",
    "しかし，まずはクラスタリングの「良さ」を明確に定義することが大事．  \n",
    "このクラスタリングの「良さ」として，sklearn.metrixに様々な指標が用意されている．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
