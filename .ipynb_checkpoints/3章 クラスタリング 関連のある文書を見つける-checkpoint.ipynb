{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前の章ではデータに正解ラベルがついていた．これを**教師あり学習** という．  \n",
    "ここでは，ラベルがない場合，**教師なし学習**を考えてみる．  \n",
    "<br>\n",
    "教師なし学習では，データだけからあるパターンを見つける．  \n",
    "例えば，質問サイトで，ある質問に関連するページを見つけるには，文書ごとの類似度を全て算出すれば良いと考えられる．  \n",
    "しかしこの方法は計算量が膨大になってしまう．  \n",
    "素早く関連する文書を見つけるためには，**クラスタリング**という手法が使えそうである．  \n",
    "こちらもテキストデータ同士で類似度を算出する方法が必要になるが，ここではSciKitライブラリの機能を使ってみたいと思う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書の関連性を計測する\n",
    "まずテキストデータを意味のある数字に変換したい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やってはいけないこと\n",
    "テキストデータの類似度を求めるには，**レーベンシュタイン距離(編集距離とも呼ばれる)** が利用できそう．  \n",
    "文字ごとの編集距離を求めると時間がかかり過ぎてしまうので，単語を最小単位として扱い，文章全体で編集距離を計測しようとも思ってみる．  \n",
    "しかし依然編集距離のオーダーは大きく，時間がかかる．  \n",
    "また，同じ単語が2つの文書に出現していて，位置が異なる場合にも編集距離は発生してしまうので，この方法はロバストであるとは言えない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どうやるべきか\n",
    "**bag-of-words**を使ってみる．  \n",
    "単語の出現回数を特徴量として扱うことで，1つの文書を1つのベクトルとして扱える．  \n",
    "(しかし，この方法で最近傍点を見つけるのには時間がかかりすぎる．)  \n",
    "**bag-of-words**によってクラスタリング処理を行う手順を示す．  \n",
    "1. 各文書から特徴量を抽出し，特徴ベクトルの形で保存する\n",
    "1. 特徴ベクトルに対してクラスタリングを行う\n",
    "1. 投稿された質問文書に対して，クラスタを決定する\n",
    "1. このクラスタに属する文書を他に集め，多様性を増す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理: 共通する単語の出現回数を類似度として計測する\n",
    "scikitのCountVectorizerでbag-of-wordsを作れる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_dfはその数より出現回数の小さい単語を無視する．  \n",
    "analizerは，単語レベルで出現回数のカウントを行なっていることを意味する．  \n",
    "token_pattenでは単語の決定方法を定義．例えばcross-validatedをcrossとvalidatedに分けるかを設定できる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]] \n",
      "\n",
      " [[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "print(\"vocabulary:\", vectorizer.get_feature_names())\n",
    "print(X.toarray(), \"\\n\\n\", X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語を数える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"This is a toy post about machine learning. Actually, it contains not much interesting stuff.\"\n",
    "txt2 = \"Imaging databases can get huge.\"\n",
    "txt3 = \"Most imaging databases safe images permanently.\"\n",
    "txt4 = \"Imaging databases store data.\"\n",
    "txt5 = \"Imaging databases store data. Imaging databases store data. Imaging databases store data.\"\n",
    "\n",
    "posts = [txt1, txt2, txt3, txt4, txt5]\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"#samples: {num_samples}, #features: {num_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
