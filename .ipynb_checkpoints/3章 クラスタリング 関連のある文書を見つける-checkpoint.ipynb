{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前の章ではデータに正解ラベルがついていた．これを**教師あり学習** という．  \n",
    "ここでは，ラベルがない場合，**教師なし学習**を考えてみる．  \n",
    "<br>\n",
    "教師なし学習では，データだけからあるパターンを見つける．  \n",
    "例えば，質問サイトで，ある質問に関連するページを見つけるには，文書ごとの類似度を全て算出すれば良いと考えられる．  \n",
    "しかしこの方法は計算量が膨大になってしまう．  \n",
    "素早く関連する文書を見つけるためには，**クラスタリング**という手法が使えそうである．  \n",
    "こちらもテキストデータ同士で類似度を算出する方法が必要になるが，ここではSciKitライブラリの機能を使ってみたいと思う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文書の関連性を計測する\n",
    "まずテキストデータを意味のある数字に変換したい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## やってはいけないこと\n",
    "テキストデータの類似度を求めるには，**レーベンシュタイン距離(編集距離とも呼ばれる)** が利用できそう．  \n",
    "文字ごとの編集距離を求めると時間がかかり過ぎてしまうので，単語を最小単位として扱い，文章全体で編集距離を計測しようとも思ってみる．  \n",
    "しかし依然編集距離のオーダーは大きく，時間がかかる．  \n",
    "また，同じ単語が2つの文書に出現していて，位置が異なる場合にも編集距離は発生してしまうので，この方法はロバストであるとは言えない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どうやるべきか\n",
    "**bag-of-words**を使ってみる．  \n",
    "単語の出現回数を特徴量として扱うことで，1つの文書を1つのベクトルとして扱える．  \n",
    "(しかし，この方法で最近傍点を見つけるのには時間がかかりすぎる．)  \n",
    "**bag-of-words**によってクラスタリング処理を行う手順を示す．  \n",
    "1. 各文書から特徴量を抽出し，特徴ベクトルの形で保存する\n",
    "1. 特徴ベクトルに対してクラスタリングを行う\n",
    "1. 投稿された質問文書に対して，クラスタを決定する\n",
    "1. このクラスタに属する文書を他に集め，多様性を増す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理: 共通する単語の出現回数を類似度として計測する\n",
    "scikitのCountVectorizerでbag-of-wordsを作れる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_dfはその数より出現回数の小さい単語を無視する．  \n",
    "analizerは，単語レベルで出現回数のカウントを行なっていることを意味する．  \n",
    "token_pattenでは単語の決定方法を定義．例えばcross-validatedをcrossとvalidatedに分けるかを設定できる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]] \n",
      "\n",
      " [[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "print(\"vocabulary:\", vectorizer.get_feature_names())\n",
    "print(X.toarray(), \"\\n\\n\", X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語を数える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"This is a toy post about machine learning. Actually, it contains not much interesting stuff.\"\n",
    "txt2 = \"Imaging databases provide storage capabilities.\"\n",
    "txt3 = \"Most imaging databases safe images permanently.\"\n",
    "txt4 = \"Imaging databases store data.\"\n",
    "txt5 = \"Imaging databases store data. Imaging databases store data. Imaging databases store data.\"\n",
    "\n",
    "posts = [txt1, txt2, txt3, txt4, txt5]\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"#samples: {num_samples}, #features: {num_features}\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# 新しい文書のベクトル化\n",
    "new_post = [\"imaging databases\"] # Iterable over raw text documents expected, string object received.対策\n",
    "new_post_vec = vectorizer.transform(new_post)\n",
    "print(new_post_vec) # 疎なベクトルなので，0でない値のみが表示される\n",
    "print(new_post_vec.toarray()) # ベクトル全体を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つの文書のユークリッド距離を計算してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist 1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def distances(posts, new_post, distance):\n",
    "    best_doc = None\n",
    "    best_dist = sys.maxsize\n",
    "    best_i = None\n",
    "    new_post_vec = vectorizer.transform(new_post)\n",
    "    for i in range(num_samples):\n",
    "        post = posts[i]\n",
    "        if post == new_post: continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        d = distance(post_vec, new_post_vec)\n",
    "        print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"Best post is %i with dist %.2f\" % (best_i, best_dist))\n",
    "\n",
    "distances(posts, new_post, dist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語の出現回数ベクトルを正規化する\n",
    "Post3とPost4の結果は同じにならないと不自然なので，正規化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist 0.77\n"
     ]
    }
   ],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要度の低い単語を取り除く\n",
    "文書の分類に貢献しない単語，**ストップワード** を取り除く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print(sorted(vectorizer.get_stop_words())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist 0.77\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステミング\n",
    "語形変化に対応し，語幹(stem)を単語として扱いたい．  \n",
    "scikitにはその機能がないため，**Natural Language Toolkit(NLTK)** が必要\n",
    "```\n",
    "$ pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphics -> graphic\n",
      "imaging -> imag\n",
      "image -> imag\n",
      "imagination -> imagin\n",
      "imagine -> imagin\n",
      "buy -> buy\n",
      "buying -> buy\n",
      "bought -> bought\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "stemmer = nltk.stem.SnowballStemmer('english') # 他にも色々ステマーはあるらしい\n",
    "\n",
    "words = [\"graphics\", \"imaging\", \"image\", \"imagination\", \"imagine\", \"buy\", \"buying\", \"bought\"]\n",
    "stemmed = [stemmer.stem(w) for w in words]\n",
    "for w, s in zip(words, stemmed): print(w, \"->\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ステミングを行い，トークン化と正規化を行うようCountVectorizerのbuild_analizerメソッドをオーバーライド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist 0.63\n"
     ]
    }
   ],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer() # 文書を小文字に変換 + 単語の抜き出し\n",
    "        return lambda doc: (stemmer.stem(w) for w in analyzer(doc)) # ステミング\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFを用いる\n",
    "頻出単語はmax_dfを0.9に設定するなどすることで除外できるが，0.89は除外できない．  \n",
    "この閾値を考えているとキリがない．  \n",
    "また，これでは除外されなかった頻出単語とそうでない単語で識別性能(クラスタリングへの貢献度)が変わってきてしまう．  \n",
    "そこで，文書内での単語の重要度として，単語の出現数の代わりに**TF-IDF(term frequency - inverse document frequency)** を使ってベクトル化する．  \n",
    "<br>\n",
    "tf: Term Frequency, 与えられた文書の中での単語の出現頻度\n",
    "$$ tf = \\frac{文書docにおける単語termの出現頻度}{文書docにおける全単語の出現頻度の和(docの単語数?)} = 文書docにおける単語termの割合$$\n",
    "<br>\n",
    "idf: Inverse Document Frequency, 逆文書頻度,単語が「レア」なら高い値を，「他の文書にも良く出現する単語」なら低い値を示すもの．  \n",
    "レアな単語はその文書の特徴を判別するのに有用であるとする．  \n",
    "$$ idf = \\log \\Bigl( \\frac{全文書数}{単語termを含む文書数} \\Bigr) = - \\log \\Bigl( \\frac{単語termを含む文書数}{全文書数} \\Bigr) $$\n",
    "これは単語termを含むという事象に対する情報量，エントロピーの式になっている．  \n",
    "この文書が単語termを含む珍しさ，含むことの予測しにくさを表している．  \n",
    "そして，TF-IDFは\n",
    "$$ tfidf = tf \\times idf $$\n",
    "で表される．  \n",
    "<br>\n",
    "TF-IDFによって，「その単語がその文書でよく出現するほど」，「その単語が他の文書と比べてレアなほど」大きな値になる．  \n",
    "つまり，TF-IDFはその単語が文書全体の中でどれだけ特徴的かを表す指標である．  \n",
    "その文書内にその単語が出現しなければ,TF-IDFは0になる．  \n",
    "参考: https://dev.classmethod.jp/machine-learning/yoshim_2017ad_tfidf_1-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term)) / sum(doc.count(w) for w in set(doc))\n",
    "    idf = math.log(float(len(docset)) / (len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.27031007207210955\n",
      "0.0\n",
      "0.13515503603605478\n",
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "doc1, doc2, doc3 = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [doc1, doc2, doc3]\n",
    "print(tfidf(\"a\", doc1, D))\n",
    "print(tfidf(\"b\", doc2, D))\n",
    "print(tfidf(\"a\", doc3, D))\n",
    "print(tfidf(\"b\", doc3, D)) # doc2におけるbはdoc3におけるbの2倍重要(2回出てきてるから)\n",
    "print(tfidf(\"c\", doc3, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearnにはTF-IDFの値で単語をベクトル化してくれるTfidfVectorizer(CountVectorizerを継承したクラス)が用意されている．  \n",
    "これをCountVectorizerの代わりに使う．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer() # 文書を小文字に変換 + 単語の抜き出し\n",
    "        return lambda doc: (stemmer.stem(w) for w in analyzer(doc)) # ステミング\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "distances(posts, new_post, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここまでやってきたこと\n",
    "1. 文書データをトークン化(bag-of-words化)する\n",
    "1. 頻出しすぎる単語は取り除く (stop wordsの除外)\n",
    "1. 滅多に使われない単語は，新しい文書でも使われる可能性が低いため，取り除く(min_dfの設定)\n",
    "1. 残った単語について，その出現回数をカウントする\n",
    "1. ↑のカウント手法だと単語ごとにクラスタリングへの貢献度が変わってきてしまうので，TF-IDFを文書の特徴ベクトルの要素とする．\n",
    "\n",
    "このbag-of-wordsによるアプローチは，単純でありながら，優れた性能を持つ．  \n",
    "しかし，以下のような欠点を持つ．  \n",
    "- 単語の前後関係について考慮していない．たとえば，Car hits wall と Wall hits carが同じものとして扱われる．\n",
    "- 否定的な意味を捉えられない． たとえば．I will eat ice creamと I will not eat ice cream の特徴ベクトルが非常に似ることになる．  \n",
    "    - この問題は，単語を個別に考えるのではなく，ペア(バイグラム)や3つの連続する単語(トリグラム)を一つのまとまりとしてカウントすることで解決できる．  \n",
    "- タイプミスに対応できない． databaseとdatabasが別の単語として扱われる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング\n",
    "ここでようやく文書の内容を適切に表現しているであろう特徴ベクトルを手にすることができた．  \n",
    "クラスタリング手法の多くは，次の2つの手法のどちらかに該当する．  \n",
    "- フラットクラスタリング(flat clustering)\n",
    "    - クラスタ間の関係を考慮しない\n",
    "    - 全てのデータがどこか1つのクラスタに属するよう分割\n",
    "    - 前もってクラスタの数を決める必要があるアルゴリズムが多い\n",
    "- 階層的クラスタリング(hierarchical clustering)\n",
    "     - 普通にフラットクラスタリングを行なった後，さらに近いクラスタ同士を一つのクラスタにまとめる親クラスタを作ることを再帰的に繰り返す\n",
    "     - 最終的には全てのデータが一つの大きなクラスタに属すことになる\n",
    "     - 前もってクラスタの数を決める必要はない． 決めることはできるが，処理効率を落とすことになるらしい\n",
    " \n",
    "Scikit-learnでできる様々なクラスタリング手法の説明  \n",
    "https://scikit-learn.org/dev/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "もっとも広く用いられるフラットクラスタリング手法\n",
    "1. クラスタの数num_clustersを指定して初期化\n",
    "1. num_clustersの数だけランダムにデータを選び出し，それらの特徴ベクトルを各クラスタの中心点とする\n",
    "1. その他のデータは，もっとも近い中心点を持つクラスタに所属させる\n",
    "1. 全てのデータのクラスタが決まったら，各クラスタの中心を新たに計算し，そこに中心点を移動させる\n",
    "1. 中心点の移動量が閾値を下回る(収束)，または指定の繰り返し回数に達するまで2~4を繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch03/plot_kmeans_example.pyを参照，実行\n",
    "![01](ch03/1400_03_01.png)\n",
    "![02](ch03/1400_03_02.png)\n",
    "![03](ch03/1400_03_03.png)\n",
    "![04](ch03/1400_03_04.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
