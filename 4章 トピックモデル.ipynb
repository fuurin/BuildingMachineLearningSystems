{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えばこの本は，「機械学習」と「Python」の両方と関係があるが，(フラット)クラスタリングではどちらか1つにしか所属できない．  \n",
    "そこで今回は，データを「トピック」と呼ばれるいくつかの小さなグループに割り当てる方法について学ぶ．  \n",
    "機械学習において，このような分野を**トピックモデル(topic modeling)**と呼ぶ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在的ディリクレ配分法(LDA, Latent Dirichlet Allocation)\n",
    "skleanのlda(Linear Discriminant Analysis, 線形判別分析)とは違うので注意  \n",
    "最も単純なトピックモデル．  \n",
    "LDAは文章製造機のようなもので，固定のトピックをいくつか持っている．  \n",
    "例えば\n",
    "- 機械学習\n",
    "- Python\n",
    "- 料理\n",
    "\n",
    "これらのトピックにはそれぞれ関連する単語のリストがある．  \n",
    "その単語の使用されている量によってトピックが決まる．  \n",
    "ここでは，この文章製造機をテキストデータの集まりから作り出し，どのようなトピックが存在するのか，そして各文書がどのトピックに割り当てられるのかを解明する．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トピックモデルの作成\n",
    "sklearnはLDAをサポートしていないので，gensimパッケージを利用する．  \n",
    "```\n",
    "$ pip install gensim\n",
    "```\n",
    "ここでは標準的なニュースレポートのデータセット，Associated Press (AP)のデータセットを用いる．  \n",
    "とりあえずデータが404になっているのでコードの実行は諦める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.BleiCorpus('./data/ap/ap.bat', './data/ap/vocab.txt') # テキストデータからBleiのLDA-C形式によるコーパスを生成\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=corpus.id2word) # コーパスからLDAモデルを取得\n",
    "topics = [model[c] for c in corpus] # モデルの各単語がどのトピックに属しているかを見る. (topic index, topic_weight)が一つのドキュメントに複数\n",
    "print(topics[0]) # 例: [(3, 0.023), (13, 0.116), (19, 0.075), (92, 0.107)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "割り当てられるトピックの数は少ないので，そのベクトルは疎なベクトルであると言える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このままだと，トピックの数は10個程度になるが，モデル生成時にalphaを設定することでその閾値を変えられる．  \n",
    "alphaは1より小さな正の値にするのが普通で，大きいとドキュメントごとのトピックは増え，小さいとドキュメントごとのトピックが減る．   \n",
    "標準では，alpha = 1.0 / len(corpus)になっている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=corpus.id2word, alpha=1) # alphaを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピックの正体は単語についての多項分布  \n",
    "各単語のそのトピックとの関連の深さを確率で表現している．  \n",
    "なので，そのトピックが何を意味しているのかは，高い確率を持つ単語のリストを提示し，人間が読み取る．  \n",
    "<br>\n",
    "この多項分布を元に，確率の高いものは大きく，そうでないものは小さく可視化した「ワードクラウド」を作り出すことができる．  \n",
    "ワードクラウドを作るサービスには，[Wordle](http://www.wordle.net/)がある．  \n",
    "WordleはWebサービスのAPIを提供しており，Pythonなどから自動でワードクラウドを作成することが可能である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トピック空間で類似度の比較を行う\n",
    "トピックは，ワードクラウドによる要約など，それだけでも実用的である．  \n",
    "<br>\n",
    "トピックを応用して，トピック空間で文書の比較を行うこともできる．  \n",
    "2つの文書が同じトピックについて論じられていれば，それは似ている文書であると考えることができる．  \n",
    "<br>\n",
    "これによって，共通する単語があまり使われていなくても，実際は同じトピックを扱っている場合にたいおうできる.  \n",
    "例えば，アメリカ大統領と記述している文書とバラク・オバマと記述している文書を似ていると判断することができる．  \n",
    "<br>\n",
    "このように，トピックモデルはそのまま可視化やデータ把握に使えるだけではなく，他の多くの課題について，中間的な役割を担うこともできる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章で取り組んだ，2つの文書の比較問題をトピックモデルで解き直す．  \n",
    "2つの文書の比較にトピックベクトルを用いる．  \n",
    "トピック数(ここでは100）は文書に使われる単語数より少ないため，ドキュメントを次元削減していることになり，これは重要なテーマである．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "if os.name == \"nt\":\n",
    "    data_home = \"D:\\Programming\\machinelearning\\BuildingMachineLearningSystems\\data\" # for windows\n",
    "else: \n",
    "    data_home = \"/Users/komatsu/programing/machinelearning/BuildingMachineLearningSystems/data\" # for Mac\n",
    "    \n",
    "categories = ['comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x','sci.space']\n",
    "data = fetch_20newsgroups(data_home=data_home, categories=categories) # 最初は数分かかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "import milk\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim import corpora, models, similarities\n",
    "import sklearn.datasets\n",
    "import nltk.stem\n",
    "from collections import defaultdict\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.update(['from:', 'subject:', 'writes:', 'writes'])\n",
    "\n",
    "\n",
    "class DirectText(corpora.textcorpus.TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        return self.input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "dataset = sklearn.datasets.load_mlcomp(\"20news-18828\", \"train\", mlcomp_root='../data')\n",
    "otexts = dataset.data\n",
    "texts = dataset.data\n",
    "\n",
    "texts = [t.decode('utf-8', 'ignore') for t in texts]\n",
    "texts = [t.split() for t in texts]\n",
    "texts = [map(lambda w: w.lower(), t) for t in texts]\n",
    "texts = [filter(lambda s: not len(set(\"+-.?!()>@012345689\") & set(s)), t) for t in texts]\n",
    "texts = [filter(lambda s: (len(s) > 3) and (s not in stopwords), t) for t in texts]\n",
    "texts = [map(english_stemmer.stem, t) for t in texts]\n",
    "usage = defaultdict(int)\n",
    "\n",
    "for t in texts:\n",
    "    for w in set(t):\n",
    "        usage[w] += 1\n",
    "\n",
    "limit = len(texts) / 10\n",
    "too_common = [w for w in usage if usage[w] > limit]\n",
    "too_common = set(too_common)\n",
    "texts = [filter(lambda s: s not in too_common, t) for t in texts]\n",
    "\n",
    "corpus = DirectText(texts)\n",
    "dictionary = corpus.dictionary\n",
    "\n",
    "try:\n",
    "    dictionary['computer']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=100, id2word=dictionary.id2token)\n",
    "\n",
    "thetas = np.zeros((len(texts), 100))\n",
    "\n",
    "for i, c in enumerate(corpus):\n",
    "    for ti, v in model[c]:\n",
    "        thetas[i, ti] += v\n",
    "\n",
    "distances = milk.unsupervised.pdist(thetas)\n",
    "large = distances.max() + 1\n",
    "for i in xrange(len(distances)):\n",
    "    distances[i, i] = large\n",
    "\n",
    "print(otexts[1])\n",
    "print()\n",
    "print()\n",
    "print(otexts[distances[1].argmin()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
