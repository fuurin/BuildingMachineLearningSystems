{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import ignore_warnings, create_or_load_pickle\n",
    "%matplotlib inline\n",
    "ignore_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えばこの本は，「機械学習」と「Python」の両方と関係があるが，(フラット)クラスタリングではどちらか1つにしか所属できない．  \n",
    "そこで今回は，データを「トピック」と呼ばれるいくつかの小さなグループに割り当てる方法について学ぶ．  \n",
    "機械学習において，このような分野を**トピックモデル(topic modeling)**と呼ぶ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "トピックモデル: データの一つ一つを「複数の」グループに割り当てる問題を扱う分野．で，文書データを分類\n",
    "- LDA(潜在的ディリクレ配分法, Latent Dirichlet Allocation): 最も単純なトピックモデル\n",
    "    - テキスト分類を考える．\n",
    "        - すべての単語に，関連するトピックを割り当てる．\n",
    "        - トピックの正体は単語についての多項分布で，各単語とそのトピックとの関連の深さを確率で表現している．\n",
    "        - ドキュメントに入っている単語のトピック強度の総和がそのドキュメントのトピック強度になる．  \n",
    "        - LDAは文章生成モデルで，あるトピックを選ぶとそのトピックのドキュメントが最もよく生成されるようにフィッティングする．\n",
    "    - 例えば，オバマと書いてあるドキュメントとトランプと書いてあるドキュメントを，トピックが似ている，と分析できる．\n",
    "    - gensimパッケージでLDAを使える．\n",
    "        - コーパスとid2wordを与えてやると学習完了．モデルにドキュメントを与えると関連するトピックが返ってくる．\n",
    "        - alphaで一つのドキュメントに割り当たるトピックの数が変わる．\n",
    "        - あるトピックが何のトピックであるのかは，そのトピックのトピック強度が強い単語を見て考えてやる．\n",
    "        - 前処理(ステミングや無関係文字の削除)を行うとなおよい\n",
    "        - alphaやトピック数といったパラメータを変えても，最終的な結果にはあまり影響がない．\n",
    "    - 学習後のモデルそのままでもwordcloudによる可視化などで役に立つ．\n",
    "    - 次元削減の効果があり，他の手法の中間的な役割を担うこともできる．\n",
    "    - 各ドキュメントのすべてのトピック強度をベクトルとすると，最もトピックが近い文章を求めることができる．\n",
    "- 階層ディリクレ過程(hierarchical Dirichlet process: HDP): トピック数をデータセットに応じて自動決定するトピックモデル\n",
    "    - 直観的には，文章が多ければ多いほど多くのトピックを得ることを利用する．\n",
    "    - 例えば，ニュースの記事データが少なければトピックは「スポーツ」になるかもしれないが，多ければ「サッカー」などと細かくできる．\n",
    "    - この手法はgensimに用意されており，先ほどのLDAをHDPに変えるだけで実現できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 潜在的ディリクレ配分法(LDA, Latent Dirichlet Allocation)\n",
    "skleanのlda(Linear Discriminant Analysis, 線形判別分析)とは違うので注意  \n",
    "最も単純なトピックモデル．  \n",
    "LDAは文章製造機のようなもので，固定のトピックをいくつか持っている．  \n",
    "例えば\n",
    "- 機械学習\n",
    "- Python\n",
    "- 料理\n",
    "\n",
    "これらのトピックにはそれぞれ関連する単語のリストがある．  \n",
    "その単語の使用されている量によってトピックが決まる．  \n",
    "ここでは，この文章製造機をテキストデータの集まりから作り出し，どのようなトピックが存在するのか，そして各文書がどのトピックに割り当てられるのかを解明する．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トピックモデルの作成\n",
    "sklearnはLDAをサポートしていないので，gensimパッケージを利用する．  \n",
    "```\n",
    "$ pip install gensim\n",
    "```\n",
    "ここでは標準的なニュースレポートのデータセット，Associated Press (AP)のデータセットを用いる．  \n",
    "次のスクリプトでダウンロードできる  \n",
    "```\n",
    "> wget http://www.cs.columbia.edu/~blei/lda-c/ap.tgz\n",
    "> tar xzf ap.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = None\n",
    "def lda_model_topics():\n",
    "    corpus = corpora.BleiCorpus('./data/ch04/ap/ap.dat', './data/ch04/ap/vocab.txt') # テキストデータからBleiのLDA-C形式によるコーパスを生成\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=corpus.id2word, alpha=alpha) # コーパスからLDAモデルを取得\n",
    "    topics = [model[c] for c in corpus] # モデルの各単語がどのトピックに属しているかを見る. (topic index, topic_weight)のリスト\n",
    "    return model, np.array(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pickle file ./data/ch04/ap/lda.pkl created.\n",
      "トピック数 100\n",
      "[(18, 0.13764551), (45, 0.01823776), (46, 0.014326233), (47, 0.04643412), (55, 0.22245163), (56, 0.17670493), (58, 0.07395291), (59, 0.02855269), (64, 0.22640407), (69, 0.017294774), (75, 0.022048322)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 76., 193., 363., 441., 428., 344., 242., 106.,  40.,  13.]),\n",
       " array([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19., 21.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, topics = create_or_load_pickle(\"./data/ch04/ap/lda.pkl\", lda_model_topics, create_new=False)\n",
    "print(\"トピック数\", model.num_topics)\n",
    "print(topics[0]) # 最初のドキュメントのトピックとトピック内での単語の重要度のリスト\n",
    "plt.hist([len(t) for t in topics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "割り当てられるトピックの数は少ないので，そのベクトルは疎なベクトルであると言える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル生成時にalphaを設定することで，今は10個程度，せいぜい25個の，1つのドキュメントに設定されるトピック数を変えられる．  \n",
    "alphaは1より小さな正の値にするのが普通で，大きいとドキュメントごとのトピックは増え，小さいとドキュメントごとのトピックが減る．   \n",
    "標準では，alpha = 1.0 / len(corpus)になっている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pickle file ./data/ch04/ap/lda_alpha1.pkl created.\n",
      "トピック数 100\n",
      "[(6, 0.0290673), (7, 0.012998671), (10, 0.0139370505), (12, 0.012141683), (21, 0.01081675), (23, 0.025263434), (29, 0.012751073), (32, 0.013778257), (39, 0.062329307), (45, 0.064363025), (49, 0.050110206), (51, 0.011268907), (65, 0.010150626), (70, 0.011367397), (74, 0.010272605), (75, 0.013880417), (77, 0.013768008), (81, 0.010769652), (82, 0.010525477), (88, 0.015850091), (91, 0.012461528), (98, 0.010918644)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 38.,  74., 214., 352., 464., 566., 324., 158.,  43.,  13.]),\n",
       " array([ 0. ,  5.2, 10.4, 15.6, 20.8, 26. , 31.2, 36.4, 41.6, 46.8, 52. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADrJJREFUeJzt3V+M3WWdx/H3Z1sUo67lz0BIW3bY2Au4WP6kIU3YCwRj+BfLhSQad21Ik96wCUY3bvXGuFkTuBFDsjFpxFg3/iMoSyNk16ZA3L0QnQryZ6uhki5MSmhd/ighskG/e3GeWccyMGc6Z3rmPPN+JZPf7/n+njm/5wmHzzz5nfP7NVWFJKlffzbuAUiSVpZBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6Serc+nEPAODss8+u6enpcQ9DkibKwYMHf11VU4v1WxVBPz09zczMzLiHIUkTJcl/D9PPSzeS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5VXFnrLRaTe++f2znPnLb9WM7t/riil6SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknq3FBBn+RIkieSPJZkptXOTLI/ydNte0arJ8mdSQ4neTzJZSs5AUnS21vKiv4DVXVJVW1t7d3AgaraAhxobYBrgS3tZxfwlVENVpK0dMu5dLMd2Nv29wI3zqt/owZ+DGxIct4yziNJWoZhg76AHyY5mGRXq51bVc8DtO05rb4ReG7e7862miRpDNYP2e+Kqjqa5Bxgf5JfvE3fLFCrN3Ua/MHYBXD++ecPOQxJ0lINtaKvqqNtewy4F7gceGHukkzbHmvdZ4HN8359E3B0gdfcU1Vbq2rr1NTUyc9AkvS2Fg36JO9O8t65feBDwJPAPmBH67YDuK/t7wM+0b59sw14Ze4SjyTp1Bvm0s25wL1J5vp/q6r+LclPgbuT7ASeBW5q/R8ArgMOA68BN4981JKkoS0a9FX1DHDxAvX/Aa5eoF7ALSMZnSRp2bwzVpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5YR9qJo3V9O77xz0EaWK5opekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6N3TQJ1mX5NEkP2jtC5I8kuTpJN9N8o5Wf2drH27Hp1dm6JKkYSxlRX8rcGhe+3bgjqraArwE7Gz1ncBLVfV+4I7WT5I0JkMFfZJNwPXAV1s7wFXAPa3LXuDGtr+9tWnHr279JUljMOyK/svAZ4A/tPZZwMtV9UZrzwIb2/5G4DmAdvyV1v9PJNmVZCbJzPHjx09y+JKkxSwa9EluAI5V1cH55QW61hDH/lio2lNVW6tq69TU1FCDlSQt3foh+lwBfDjJdcDpwJ8zWOFvSLK+rdo3AUdb/1lgMzCbZD3wPuDFkY9ckjSURVf0VfXZqtpUVdPAR4EHq+rjwEPAR1q3HcB9bX9fa9OOP1hVb1rRS5JOjeV8j/4fgE8lOczgGvxdrX4XcFarfwrYvbwhSpKWY5hLN/+vqh4GHm77zwCXL9Dnd8BNIxibJGkEvDNWkjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ1b0kPNpOnd9497CJKWyBW9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zjtjpVVqXHchH7nt+rGcVyvHFb0kdc6gl6TOGfSS1DmDXpI6t2jQJzk9yU+S/DzJU0m+0OoXJHkkydNJvpvkHa3+ztY+3I5Pr+wUJElvZ5gV/evAVVV1MXAJcE2SbcDtwB1VtQV4CdjZ+u8EXqqq9wN3tH6SpDFZNOhr4NXWPK39FHAVcE+r7wVubPvbW5t2/OokGdmIJUlLMtQ1+iTrkjwGHAP2A78CXq6qN1qXWWBj298IPAfQjr8CnDXKQUuShjdU0FfV76vqEmATcDlw4ULd2nah1XudWEiyK8lMkpnjx48PO15J0hIt6Vs3VfUy8DCwDdiQZO7O2k3A0bY/C2wGaMffB7y4wGvtqaqtVbV1amrq5EYvSVrUMN+6mUqyoe2/C/ggcAh4CPhI67YDuK/t72tt2vEHq+pNK3pJ0qkxzLNuzgP2JlnH4A/D3VX1gyT/BXwnyT8BjwJ3tf53Af+S5DCDlfxHV2DckqQhLRr0VfU4cOkC9WcYXK8/sf474KaRjE6StGzeGStJnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdW79uAegpZveff+4hyBpgiy6ok+yOclDSQ4leSrJra1+ZpL9SZ5u2zNaPUnuTHI4yeNJLlvpSUiS3towl27eAD5dVRcC24BbklwE7AYOVNUW4EBrA1wLbGk/u4CvjHzUkqShLRr0VfV8Vf2s7f8WOARsBLYDe1u3vcCNbX878I0a+DGwIcl5Ix+5JGkoS/owNsk0cCnwCHBuVT0Pgz8GwDmt20bguXm/NttqkqQxGDrok7wH+B7wyar6zdt1XaBWC7zeriQzSWaOHz8+7DAkSUs0VNAnOY1ByH+zqr7fyi/MXZJp22OtPgtsnvfrm4CjJ75mVe2pqq1VtXVqaupkxy9JWsQw37oJcBdwqKq+NO/QPmBH298B3Dev/on27ZttwCtzl3gkSafeMN+jvwL4W+CJJI+12ueA24C7k+wEngVuasceAK4DDgOvATePdMSSpCVZNOir6j9Z+Lo7wNUL9C/glmWOS5I0Ij4CQZI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1Ln/DdjJf2Jcf6bxEduu35s5+6ZK3pJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknq3KJBn+RrSY4leXJe7cwk+5M83bZntHqS3JnkcJLHk1y2koOXJC1umBX914FrTqjtBg5U1RbgQGsDXAtsaT+7gK+MZpiSpJO1aNBX1Y+AF08obwf2tv29wI3z6t+ogR8DG5KcN6rBSpKW7mSv0Z9bVc8DtO05rb4ReG5ev9lWe5Mku5LMJJk5fvz4SQ5DkrSYUX8YmwVqtVDHqtpTVVurauvU1NSIhyFJmnOyQf/C3CWZtj3W6rPA5nn9NgFHT354kqTlOtmg3wfsaPs7gPvm1T/Rvn2zDXhl7hKPJGk81i/WIcm3gSuBs5PMAp8HbgPuTrITeBa4qXV/ALgOOAy8Bty8AmOWJC3BokFfVR97i0NXL9C3gFuWOyhJ0uh4Z6wkdc6gl6TOGfSS1DmDXpI6t+iHsXpr07vvH/cQJGlRruglqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc5HIEhaNcb1WJEjt10/lvOeKq7oJalzE7+i98FikvT2XNFLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOjfxN0xJ0nKN88bLU/H4hRVZ0Se5JskvkxxOsnslziFJGs7Igz7JOuCfgWuBi4CPJblo1OeRJA1nJVb0lwOHq+qZqvpf4DvA9hU4jyRpCCsR9BuB5+a1Z1tNkjQGK/FhbBao1Zs6JbuAXa35apJfnuT5zgZ+fZK/O2nWylzXyjxh7cx1rcwTljjX3L6sc/3FMJ1WIuhngc3z2puAoyd2qqo9wJ7lnizJTFVtXe7rTIK1Mte1Mk9YO3NdK/OE1TnXlbh081NgS5ILkrwD+CiwbwXOI0kawshX9FX1RpK/A/4dWAd8raqeGvV5JEnDWZEbpqrqAeCBlXjtBSz78s8EWStzXSvzhLUz17UyT1iFc03Vmz4nlSR1xGfdSFLnJjroe37UQpKvJTmW5Ml5tTOT7E/ydNueMc4xjkKSzUkeSnIoyVNJbm31ruaa5PQkP0ny8zbPL7T6BUkeafP8bvsCQxeSrEvyaJIftHZ3c01yJMkTSR5LMtNqq+69O7FBvwYetfB14JoTaruBA1W1BTjQ2pPuDeDTVXUhsA24pf137G2urwNXVdXFwCXANUm2AbcDd7R5vgTsHOMYR+1W4NC8dq9z/UBVXTLvK5Wr7r07sUFP549aqKofAS+eUN4O7G37e4EbT+mgVkBVPV9VP2v7v2UQDBvpbK418GprntZ+CrgKuKfVJ36ec5JsAq4HvtraodO5LmDVvXcnOejX4qMWzq2q52EQkMA5Yx7PSCWZBi4FHqHDubZLGY8Bx4D9wK+Al6vqjdalp/fwl4HPAH9o7bPoc64F/DDJwXa3P6zC9+4kP49+qEctaDIkeQ/wPeCTVfWbwQKwL1X1e+CSJBuAe4ELF+p2akc1ekluAI5V1cEkV86VF+g68XMFrqiqo0nOAfYn+cW4B7SQSV7RD/Wohc68kOQ8gLY9NubxjESS0xiE/Der6vut3OVcAarqZeBhBp9JbEgyt+Dq5T18BfDhJEcYXFK9isEKv7u5VtXRtj3G4I/35azC9+4kB/1afNTCPmBH298B3DfGsYxEu3Z7F3Coqr4071BXc00y1VbyJHkX8EEGn0c8BHykdZv4eQJU1WeralNVTTP4//LBqvo4nc01ybuTvHduH/gQ8CSr8L070TdMJbmOwUph7lELXxzzkEYmybeBKxk8Ce8F4PPAvwJ3A+cDzwI3VdWJH9hOlCR/DfwH8AR/vJ77OQbX6buZa5K/YvDB3DoGC6y7q+ofk/wlg1XvmcCjwN9U1evjG+lotUs3f19VN/Q21zafe1tzPfCtqvpikrNYZe/diQ56SdLiJvnSjSRpCAa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0md+z/VR9RUoJHosQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "model, topics = create_or_load_pickle(\"./data/ch04/ap/lda_alpha1.pkl\", lda_model_topics, create_new=False)\n",
    "print(\"トピック数\", model.num_topics)\n",
    "print(topics[0])\n",
    "plt.hist([len(t) for t in topics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピック数が30個程度になった．  \n",
    "<br>\n",
    "トピックの正体は単語についての多項分布  \n",
    "各単語のそのトピックとの関連の深さを確率で表現している．  \n",
    "なので，そのトピックが何を意味しているのかは，高い確率を持つ単語のリストを提示し，人間が読み取る．  \n",
    "次のように文書ごとのトピックを表示してみると，トピック間の関連性が見えてくる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ch04/ap/lda.pkl was loaded from the pickle file.\n",
      "bush market report department reported make office get four defense foreign\n",
      "year last billion today national friday office get world work five stock campaign\n",
      "people years soviet thursday reported make get spokesman back defense trade workers\n",
      "percent people year last years united bush thursday house wednesday market military reported going get committee defense days asked\n",
      "report department committee case\n",
      "percent people billion thursday get world work defense foreign economic campaign case\n",
      "years state told monday market department world prices workers administration\n",
      "new united defense\n",
      "last nations\n",
      "years bush house defense stock law months case\n"
     ]
    }
   ],
   "source": [
    "model, topics = create_or_load_pickle(\"./data/ch04/ap/lda.pkl\", lda_model_topics, create_new=False)\n",
    "for topic in topics[:10]:\n",
    "    print(*[model.id2word[i] for i, _ in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに，一般的な単語(ストップワード)を削除したり，前処理として言葉の変化形を同一単語として扱うステミングを行うと精度が上がる．  \n",
    "これは前章で行っているので省略．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力された多項分布を元に，確率の高いものは大きく，そうでないものは小さく可視化した「ワードクラウド」を作り出すことができる．  \n",
    "ワードクラウドを作るサービスには，[Wordle](http://www.wordle.net/)がある．  \n",
    "WordleはWebサービスのAPIを提供しており，Pythonなどから自動でワードクラウドを作成することが可能である．  \n",
    "wordcloudパッケージをpipで入れればよい．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トピック空間で類似度の比較を行う\n",
    "トピックは，ワードクラウドによる要約など，それだけでも実用的である．  \n",
    "<br>\n",
    "トピックを応用して，トピック空間で文書の比較を行うこともできる．  \n",
    "2つの文書が同じトピックについて論じられていれば，それは似ている文書であると考えることができる．  \n",
    "<br>\n",
    "これによって，共通する単語があまり使われていなくても，実際は同じトピックを扱っている場合にたいおうできる.  \n",
    "例えば，アメリカ大統領と記述している文書とバラク・オバマと記述している文書を似ていると判断することができる．  \n",
    "<br>\n",
    "このように，トピックモデルはそのまま可視化やデータ把握に使えるだけではなく，他の多くの課題について，中間的な役割を担うこともできる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章で取り組んだ，2つの文書の比較問題をトピックモデルで解き直す．  \n",
    "2つの文書の比較にトピックベクトルを用いる．  \n",
    "これによって，ある文章にトピックが似ている文章を選択することを目指す．  \n",
    "<br>\n",
    "なお，トピック数(ここでは100）は文書に使われる単語数より少ないため，ドキュメントを次元削減していることになる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skleanによるデータの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','comp.windows.x','sci.space']\n",
    "dataset = fetch_20newsgroups(data_home=\"./data/ch04\", categories=categories) # 最初は数分かかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./data/ch04...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', download_dir=\"./data/ch04\")\n",
    "nltk.data.path.append(\"./data/ch04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def preprocess_text():\n",
    "    english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    stopwords.update(['from:', 'subject:', 'writes:', 'writes'])\n",
    "\n",
    "    texts = dataset.data # 前処理用データ\n",
    "    \n",
    "    texts = [t.encode('ascii', 'ignore').decode('utf-8', 'ignore') for t in texts] # エンコードできない文字を無視\n",
    "    texts = [t.split() for t in texts] # 単語に分割\n",
    "    texts = [map(lambda w: w.lower(), t) for t in texts] # 小文字に変換\n",
    "    \n",
    "    # イテレータは一度参照するとすべてを出力し尽くして空になる．なのでlistに変換しておく．\n",
    "    texts = [list(filter(lambda s: len(set(\"+-.?!()>@012345689\") & set(s)) == 0, t)) for t in texts] # 数字記号を含む単語を削除    \n",
    "    texts = [list(filter(lambda s: (len(s) > 3) and (s not in stopwords), t)) for t in texts] # stopwordに含まれない4文字以上の単語に限定\n",
    "    texts = [list(map(english_stemmer.stem, t)) for t in texts] # ステミング\n",
    "\n",
    "    # 全コーパスの単語リストを作成\n",
    "    usage = defaultdict(int)\n",
    "    for t in texts:\n",
    "        for w in set(t):\n",
    "            usage[w] += 1\n",
    "\n",
    "    # 10%以上のドキュメントで使われている単語を排除\n",
    "    limit = len(texts) / 10\n",
    "    too_common = set([w for w in usage if usage[w] > limit])\n",
    "    texts = [list(filter(lambda s: s not in too_common, t)) for t in texts]\n",
    "    \n",
    "    return [list(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ch04/preprocessed_20news.pkl was loaded from the pickle file.\n",
      "3529 ['general', 'inform', 'request', 'worcest', 'polytechn', 'institute,', 'worcester,', 'inform', 'includ', 'nasa,', 'shuttles,', 'history,', 'suggest', 'books,', 'periodicals,', 'site', 'novic', 'interest', 'todd', 'giaquinto']\n"
     ]
    }
   ],
   "source": [
    "otexts = dataset.data # 元データ\n",
    "texts = create_or_load_pickle(\"./data/ch04/preprocessed_20news.pkl\", preprocess_text, create_new=False) # 前処理済みデータ\n",
    "\n",
    "# 前処理したデータをコーパスにする．コーパスの長さと文字列を得られるようにする．\n",
    "class DirectText(corpora.textcorpus.TextCorpus):\n",
    "    def get_texts(self):\n",
    "        return self.input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "corpus = DirectText(texts)\n",
    "\n",
    "print(len(corpus), corpus.get_texts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus.get_texts())):\n",
    "    if len(corpus.get_texts()[i]) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理したテキストをLDAモデルにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(corpus, num_topics=100, id2word=corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAモデルから全てのドキュメント同士のトピックベクトルの距離を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pickle file ./data/ch04/distances_20news.pkl created.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def make_distances():\n",
    "    # 全てのドキュメントについて，100個あるトピックのどれがどれだけ強いかをリストにする\n",
    "    dense = np.zeros((len(texts), 100))\n",
    "    for i, c in enumerate(corpus):\n",
    "        for ti, v in model[c]: # topic index, topic strength\n",
    "            dense[i, ti] += v\n",
    "\n",
    "    distances = distance.pdist(dense) # 全てのドキュメントの組み合わせの距離を求める．自分自身との距離はない\n",
    "    distances = distance.squareform(distances) # 距離のリストを2次元の距離行列に変換する．対各成分はこのとき0\n",
    "    distances[np.identity(len(distances), dtype=bool)] = distances.max() + 1 # 対各成分，つまり自分自身との距離は最大にする\n",
    "    return distances\n",
    "\n",
    "distances = create_or_load_pickle(\"./data/ch04/distances_20news.pkl\", make_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: auerbach@batman.bmd.trw.com\n",
      "Subject: Accelerating the MacPlus...;)\n",
      "Lines: 15\n",
      "\n",
      "We're about ready to take a bold step into the 90s around here by accelerating\n",
      "our rather large collection of stock MacPlus computers. Yes indeed, difficult\n",
      "to comprehend why anyone would want to accelerate a MacPlus, but that's another\n",
      "story. Suffuce it to say, we can get accelerators easier than new machines.\n",
      "Hey, I don't make the rules...\n",
      "\n",
      "Anyway, on to the purpose of this post: I'm looking for info on MacPlus\n",
      "acelerators.  So far, I've found some lit on the Novy Accelerator and the\n",
      "MicrMac MultiSpeed Accelartor. Both look acceptable, but I would like to hear\n",
      "from anyone who has tried these.  Also, if someone would recommend another\n",
      "accelerator for the MacPlus, I'd like to hear about it.\n",
      "\n",
      "Thanks for any time and effort you expend on this!\n",
      "\n",
      "Karl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From: claebaur@shell.portal.com (Tony S Annese)\n",
      "Subject: Re: Utility for updating Win.ini and system.ini\n",
      "Nntp-Posting-Host: jobe\n",
      "Organization: Portal Communications Company -- 408/973-9111 (voice) 408/973-8091 (data)\n",
      "Lines: 10\n",
      "\n",
      "In article <1993Apr20.130933.26571@lut.ac.uk> A.D.Bailey@lut.ac.uk (Adrian D.Bailey) writes:\n",
      ">In Windows 3.0 there is a built-in called sysedit.exe that is just what you\n",
      ">need.   Windows 3.1 I don't know......\n",
      "\n",
      "It's there.....\n",
      "--\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n",
      "Tony Annese                                    claebaur@shell.portal.com\n",
      "                                          -or- claebaur@cup.portal.com\n",
      "\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(otexts[1])\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(otexts[distances[1].argmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように，似たような話題(コンピュータ系?)の文書を見つけることができた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia全体のモデル化\n",
    "Wikipediaのダンプデータを使ってトピックモデルを作成してみる．  \n",
    "まずは巨大なデータをダウンロード  \n",
    "\n",
    "``` bash\n",
    "> wget http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 --no-check-certificate\n",
    "```\n",
    "さすがに5~10時間かかるので実行はあきらめる．  \n",
    "ダウンロードが完了したら，gensimのツールを使ってファイルのインデックス化を行う．  \n",
    "これにもまた数時間かかる．  \n",
    "``` bash\n",
    "> python -m gensim.scripts.make_wiki enwiki-latest-pages-articles.xml.bz2 wiki_en_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "処理時間がかかる場合は，loggingモジュールで表示を行う．  \n",
    "- asctime: イベントの日付と時刻\n",
    "- levelname: INFOとかDEBUGとか\n",
    "- message: logging.info(message)で出力されるメッセージ\n",
    "- level: 書き出しの最低レベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理を行ったデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_output_wordids.txt')\n",
    "mm = gensim.corpora.MmCorpus('wiki_en_output_tfidf.mm') # 疎な行列のフォーマットにMatrix Marketというものがある．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に，LDAモデルを作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=mm,\n",
    "    id2word=id2word,\n",
    "    num_topics=100, # デフォルトだけどね\n",
    "    chunksize=10000, # 訓練で用いるデータの数\n",
    ")\n",
    "model.save('wiki.lda.pkl') # 時間がかかる作業なので結果を保存しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.ldamodel.LdaModel.load('wiki_lda.pkl') # ロードはこう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではトピックの中身を見ていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "topics = [model[doc] for doc in mm] # docの数は400万以上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピック数の平均を見てみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "lens = np.array([len(t) for t in topics])\n",
    "print(np.mean(lens)) # 6.5個程度\n",
    "print(np.mean(lens <= 10)) # 93%程度の文章が10個以下のトピックを持つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに，Wikipediaで最も多く述べられているトピックについて調べられる．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# どのトピックがどれだけのドキュメントに関係しているかを数える\n",
    "counts = np.zeros(100)\n",
    "for doc_top in topics:\n",
    "    for ti, _ in doc_top:\n",
    "        counts[ti] += 1\n",
    "words = model.show_topic(counts.argmax(), 64) # 最も多く書かれているトピックに強く関連する単語64個を取得\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最も多く書かれているトピックはbook, movie, fiction, storyなどの物語に関すること．(2013年)  \n",
    "Wikipedia全体の25%がこのトピックと関連している．  \n",
    "また，5%の単語はこのトピックから生成される．  \n",
    "<br>\n",
    "逆に，記述の少ないトピックを見てみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "words = model.show_topic(counts.argmin(), 64) # 最も多く書かれているトピックに強く関連する単語64個を取得\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中央アフリカのフランス植民地，など"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピックの数を選択する\n",
    "LDAでは抽出するトピックの数を選べたり，alphaで一つのドキュメントのトピック数を変えられる．  \n",
    "しかしこれらのパラメータはあまり意味がなく，中間的なツールとして利用したときも最終的な結果にはあまり影響を及ぼさない．  \n",
    "<br>\n",
    "抽出するトピックの数をデータセットに応じて決定する手法の一つに，階層ディリクレ過程(hierarchical Dirichlet process: HDP)がある．  \n",
    "直観的には，文章が多ければ多いほど多くのトピックを得る．  \n",
    "例えば，ニュースの記事データが少なければトピックは「スポーツ」になるかもしれないが，多ければ「サッカー」などと細かくできる．  \n",
    "この手法はgensimに用意されており，先ほどのLDAをHDPに変えるだけで実現できる．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "hdp = gensim.models.hdpmodel.HdpModel(mm, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピックモデルの研究には，他にもIndian buffet processやPachinko allocationなど面白い名前の手法がある．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
